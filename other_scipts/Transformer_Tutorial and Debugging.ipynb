{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8d87ca",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc25c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd36d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238276ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3589575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102499, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    9,    59,   564,   223,   443, 13627,     2,   539,  2872,  2464,\n",
       "             0,   313,  4513,     1,     5,    47,    66, 11652,  2435,     1],\n",
       "        [ 3849,    12,   300,  6302,  3989,  1930, 10559,   451,     4,     7,\n",
       "             2,  1511, 10115,   942,  2439,   572,     1,    47,    30,  1990],\n",
       "        [ 3869,   315,    19,    29,   939,     2,    10,  2139,  4916, 16615,\n",
       "           235,     3,    13,     7,    24,    17, 13737,    97,  7720,     4],\n",
       "        [  881,    67,   807,  5402,     6,    38, 28188,    25,     2,    77,\n",
       "             7,  2394,    17,   516,    14, 16403,  3714,  4618,    12,  1108],\n",
       "        [    9,   196,  6041,   190,   218, 11776,    17,     1,  1200,     2,\n",
       "             0,    10,   591,    40,  6004,     2,    50,     3,  3131,  3781]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509a2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21441, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    9,     7,  1934,   340,     3,     1,   148,  2154,   349,    32],\n",
       "        [ 9606,     1, 18113,     7,     9,   133,     7,   126,    51,  3490],\n",
       "        [25610,   110,   256,  7570,  1004,  3660,   845,    20,     8,    56],\n",
       "        [    9,   186,    11,     1,     6,    16,     1,  2877,   654,  1748],\n",
       "        [ 9606,  3049,    15,   146,  1289,     2,    67,   339,     4,     3]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(val_data.shape)\n",
    "val_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3119550",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b6e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30bf01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28782"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4463008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20, 200])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[:35].shape)\n",
    "model.encoder(train_data[:35]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff587a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 1, 200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pos_encoder.pe[:35].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a7d88ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7289,  1.8346,  0.4786,  ...,  1.4639, -1.4171,  0.0813],\n",
       "         [-0.8794, -0.1175,  1.4903,  ...,  0.2706, -0.8367, -0.3261],\n",
       "         [ 0.0000,  0.9132,  0.4607,  ...,  0.3973,  0.4494,  1.7791],\n",
       "         ...,\n",
       "         [ 0.0650,  0.0000,  1.1919,  ...,  0.0000, -1.4340,  0.3072],\n",
       "         [-1.2031, -0.4692,  1.4120,  ...,  0.4051, -1.3769,  2.4646],\n",
       "         [ 0.8749,  0.7204,  1.2893,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.4131,  0.0000,  0.0000,  ...,  2.6297, -1.5589,  0.2041],\n",
       "         [ 0.0000,  0.0508,  0.7145,  ...,  1.1957, -1.7035, -0.1074],\n",
       "         [ 1.2993,  0.1980,  0.0000,  ...,  0.2091,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.7394,  0.0000,  1.2292,  ...,  1.1448, -0.1162,  0.0000],\n",
       "         [ 1.8246,  0.0000,  2.4204,  ...,  0.1613,  0.7780,  0.0000],\n",
       "         [ 1.5124,  1.2966,  1.1471,  ...,  1.2086, -1.3884,  1.3919]],\n",
       "\n",
       "        [[ 0.0000, -0.9334, -0.5322,  ...,  1.8138,  0.5712,  0.0000],\n",
       "         [ 2.5898, -0.0424,  0.5627,  ...,  1.1992,  0.9277,  0.0000],\n",
       "         [ 1.9656, -0.1543,  1.4358,  ...,  0.2802,  0.0000,  0.9570],\n",
       "         ...,\n",
       "         [ 2.1778,  0.6477,  1.2961,  ...,  0.4442,  0.0000,  2.8495],\n",
       "         [ 1.8233, -0.1554,  2.9118,  ...,  0.4246,  0.5021,  1.3145],\n",
       "         [ 2.2666, -1.0581,  2.0663,  ...,  0.1993,  0.6628,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2079,  2.0725, -0.4389,  ...,  1.5431,  1.1459,  0.2755],\n",
       "         [ 2.4425, -0.4250, -2.5398,  ...,  2.4315,  0.0000, -0.2978],\n",
       "         [ 1.4621,  0.0000,  0.4451,  ...,  0.0000,  0.7823, -0.3076],\n",
       "         ...,\n",
       "         [-0.1724,  2.1998, -0.3730,  ...,  0.9142,  0.0000,  0.5808],\n",
       "         [ 0.0000,  2.4885, -1.9447,  ...,  1.4384,  0.6524,  1.0064],\n",
       "         [ 0.1966,  2.8073, -1.7594,  ...,  0.8975,  1.6663,  0.2457]],\n",
       "\n",
       "        [[ 0.0000,  0.0000, -1.9833,  ...,  0.8975,  1.6664,  0.2456],\n",
       "         [-0.1763, -0.5641, -2.7059,  ...,  0.0000, -0.1183,  0.0000],\n",
       "         [-0.5061,  0.4093, -0.5635,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.3417, -1.3163, -0.4161,  ...,  2.1521,  0.1516,  0.0000],\n",
       "         [ 0.8673,  1.1291,  0.2793,  ...,  0.0000,  0.9100,  0.0780],\n",
       "         [-0.2599,  0.0000, -1.2446,  ...,  2.2006, -1.3681,  1.4147]],\n",
       "\n",
       "        [[ 1.6606, -0.6153,  0.0000,  ...,  2.0488, -0.7098,  0.0000],\n",
       "         [ 1.7914, -1.5987,  0.3607,  ...,  0.1993,  0.6671,  0.1464],\n",
       "         [ 1.7914, -1.5987,  0.3607,  ...,  0.1993,  0.6671,  0.0000],\n",
       "         ...,\n",
       "         [ 1.4170, -0.0680, -1.6790,  ...,  0.0000,  1.3734,  0.4552],\n",
       "         [ 1.7275, -1.9899, -0.2644,  ...,  1.7199,  1.0215,  1.4182],\n",
       "         [ 0.2647, -2.4729, -1.7444,  ...,  2.0820, -0.7058,  0.0000]]],\n",
       "       device='cuda:0', grad_fn=<FusedDropoutBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pos_encoder(model.encoder(train_data[:35]) * math.sqrt(model.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57242b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b130947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2adc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e1404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429919e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdfb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b32fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6a3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04febf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "#         em_src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        pos_src = self.pos_encoder(src * math.sqrt(self.d_model))\n",
    "        output_trans = self.transformer_encoder(pos_src, src_mask)\n",
    "        output_final = self.decoder(output_trans)\n",
    "        return pos_src, output_trans, output_final\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae97558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62795b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e82eea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2049990]), 'torch.LongTensor')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_data.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f1626fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1df04bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102499, 20])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f375b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4103e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 1024  # size of vocabulary\n",
    "d_model = 768  # embedding dimension\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "d_hid = 768  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, d_model, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2744930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "#     data, targets = get_batch(train_data, i)\n",
    "#     print(data.size())\n",
    "#     print(targets.size())\n",
    "#     batch_size = data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746f0aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = len(train_data) // bptt\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07b86cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_data[:35].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_dataset[0]['sentence_embeddings'][:40].type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c067e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_mask = generate_square_subsequent_mask(35).to(device)\n",
    "# model.train()\n",
    "# em_src, pos_src, output = model(train_data[:35], None)  # transformed_dataset[0]['sentence_embeddings'][:35].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc648e2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    9,    59,   564,   223,   443, 13627,     2,   539,  2872,  2464,\n",
       "             0,   313,  4513,     1,     5,    47,    66, 11652,  2435,     1],\n",
       "        [ 3849,    12,   300,  6302,  3989,  1930, 10559,   451,     4,     7,\n",
       "             2,  1511, 10115,   942,  2439,   572,     1,    47,    30,  1990],\n",
       "        [ 3869,   315,    19,    29,   939,     2,    10,  2139,  4916, 16615,\n",
       "           235,     3,    13,     7,    24,    17, 13737,    97,  7720,     4],\n",
       "        [  881,    67,   807,  5402,     6,    38, 28188,    25,     2,    77,\n",
       "             7,  2394,    17,   516,    14, 16403,  3714,  4618,    12,  1108],\n",
       "        [    9,   196,  6041,   190,   218, 11776,    17,     1,  1200,     2,\n",
       "             0,    10,   591,    40,  6004,     2,    50,     3,  3131,  3781],\n",
       "        [20000,    21,  6406,    14,     3,    10,     8,   114,    24,  2294,\n",
       "          1684,  7156,  1681,   191,   928,  5879,    51,     9,  1147,     5],\n",
       "        [   83,     8,     2,  1019,  1817,    39,   971,  2581,     1, 12522,\n",
       "             2,    19,     7,    26,     2,     5,  1104,     9,  2995,   986],\n",
       "        [ 3849,   236,    52,  5067,    50,     3,   818,   131,  1314,    13,\n",
       "            10,  5136,   481,     1,    53,  9503,     7,  9584, 10187,    18],\n",
       "        [   88,   152,  3054,    29,   214,   130,     2,     5,  8211,     8,\n",
       "           121,    11,     3,   345,    10,     3,     0,     9,     2,    27],\n",
       "        [    0,  1526,   265,   591,     8,  4111,  1656,   841,  7129,     0,\n",
       "           123,    15,   104,     3, 19129,    69,    27,     9,   772,  4637],\n",
       "        [ 3869,    19,     1,    13,   620,    11,    80,     8,     6,     0,\n",
       "            14,   589,     2,    18,    73,  1012,   138,  1687,    19, 25204],\n",
       "        [   21, 17322,  4347,     1,   299,    15,     4,   227,  1787,    13,\n",
       "             8,     2,    14,     1, 12294,  2706,     3,    50, 14545,     2],\n",
       "        [  780, 10360,     4,   519,     6,   600,     1,     0,     7,    32,\n",
       "           508,     5,     1,    68,     7,   483,     6,     8,    26,     5],\n",
       "        [28780,    20,   695,    47,     1,   254, 13912,   470,     1,  1450,\n",
       "             5,   842,   659,  1664,     1,   209,     1,   965,   160,   279],\n",
       "        [    2,     3,     5,   306,  1247,     1,    16,    18, 12103,    10,\n",
       "            44,    77,     4,   619,   247,   569,   142,     4,   273,   112],\n",
       "        [ 6182,    16,  1226,   330,   166,  2450,    28,  1549,  4659,   661,\n",
       "            17,    79,     1,     6,    11,     3,     2,   204,   177,     0],\n",
       "        [    3,    85, 14195,     3,     4,    53,    31, 18490,     3,     2,\n",
       "            43,    10,   702,     1,    15,    25,  2361,    22,     3,     1],\n",
       "        [ 3849,     2,     6,  5695,     1,   296,  1781,     5,   813,     5,\n",
       "          7089,    83,  2871,   237,  6437,    26,   537,     3,  1190,  2286],\n",
       "        [    4,   665,  8138,    13,   291,  6723,    18,     0,     1,  6959,\n",
       "         22540,  1366,     2,   552,  2641,   561,  2649,    22,   145,     4],\n",
       "        [    1,    10,     3,     1,   152,    94,     1, 22618,   476,    46,\n",
       "            56,    17,  2135,    13,  3779,     1,  4990,  1593,   447,    35],\n",
       "        [ 5023,     1,  1850,   291,  1146,     3,   479,     0,   198,   137,\n",
       "             1,    77,    31,     8,    14,  7611,    21,  2869,  3294,  3979],\n",
       "        [   88,    68,     4,  8102,   233,   569,   231,     8,     4,    16,\n",
       "             0,     7,   793,  1809,    24,     2, 14594,     0,    23,     5],\n",
       "        [   20,  1646,     1,     7,    21,    90,     3,     0,     1,    28,\n",
       "          2092,    11,    16,    12,    62,  9503, 15971,  1750,    16,  6907],\n",
       "        [    2,   285,    54,   215,     5,     1,  2854,     5,   208,   147,\n",
       "             3, 15262,    28,   677,   347,  2515,    20,    14,    64,  6450],\n",
       "        [ 1837,     7,  1846,   109,   152,  1143,   118, 19726,     2,  1206,\n",
       "          6401,     6,    10,  8365,    58,     1,     5,  1715,  1014,     4],\n",
       "        [ 1018,   231,    10,    22,     1,     4,    10, 22549,    18,    13,\n",
       "            12,    11,  8125,   370,   409,  1405,   741,    25,     8,   494],\n",
       "        [    7,     6,  2959,     3,  2064, 11776,  1162,     3,  1245,   958,\n",
       "             0,  1547,    19,   622,    25,   440, 19354,     1,   875,   935],\n",
       "        [   14,    59,     6,    22,   131,     5,     7,     1, 10944,     3,\n",
       "           536,    17,    27,     2,  4604,    10,    21,  3481,  1057,     3],\n",
       "        [ 3849,  4309,   960,    88,  9695,  4772,  3501,  1982,     2,   174,\n",
       "           679,    27,   920,  5317,    46,  7665,   170,  4206,    81,     6],\n",
       "        [ 3869,   193,     2,   164,     2,     2,    74,   726,    23,  8525,\n",
       "          3007,   277,    11,   635,   545,    36, 10472,     3,  3521,  3470],\n",
       "        [  881,     2,    79,     3,    50,   104,   174,    23,  4916,   806,\n",
       "            14,    11,    15,     1,     3,  1722,    20,    18,  3160, 16017],\n",
       "        [  629,  1042,    10,     6,    51,     2,    37,  1622,  7129,     6,\n",
       "         27178, 22979, 19202,  4966,  5157,  4048,   138,    35,   147,  1377],\n",
       "        [  976,     6,    30,     1,     1, 11776,   190,    19,     0,     1,\n",
       "             2,    11,     5,     7,    10,    10, 13217,   965,    49,     2],\n",
       "        [    2,  4465, 10537,     0,  1693,    11,     5,     1,     1,   142,\n",
       "             8,     3,  2080,   640,    41,     7,     6,  1687,   136,    57],\n",
       "        [   23,     4,     4,     0,     4,    15,   520,   730,   208,  9518,\n",
       "           223,  3506,    29,    40,     4,  5686,    30,  6579,    24,    64]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd4329",
   "metadata": {},
   "source": [
    "## Debug Positional Encoding with my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "573cd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(5000).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, 768, 2) * (-math.log(10000.0) / 768))\n",
    "pe = torch.zeros(5000, 1, 768)\n",
    "pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 0, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "935b1f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e9a6c",
   "metadata": {},
   "source": [
    "#### remember to convert the type to FloatTensor => .type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85129a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0131,  0.0572, -0.0360,  ..., -0.0415, -0.0471,  0.0127],\n",
       "        [-0.0290,  0.0563, -0.0076,  ..., -0.0085, -0.0270, -0.0027],\n",
       "        [ 0.0337, -0.0226, -0.0394,  ...,  0.0040, -0.0337, -0.0060],\n",
       "        ...,\n",
       "        [-0.0279,  0.0281,  0.0023,  ..., -0.0149,  0.0905, -0.0053],\n",
       "        [-0.0013,  0.0304, -0.0268,  ..., -0.0260,  0.0886, -0.0083],\n",
       "        [ 0.0515, -0.0140, -0.0373,  ...,  0.0157,  0.0309,  0.0181]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset[0]['sentence_embeddings'].type(torch.FloatTensor)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c27ecf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "377903ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transformed_dataset[0]['sentence_embeddings'].type(torch.FloatTensor)[:1] + pe[:1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846dec2",
   "metadata": {},
   "source": [
    "### Debug: give the dataset to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9aaa72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_square_subsequent_mask(50).to(device)\n",
    "model.train()\n",
    "pos_src, output_trans, output_final = model(transformed_dataset[0]['sentence_embeddings'].type(torch.FloatTensor)[:50].to(device),\n",
    "                                            src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc897bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a110be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "print(pos_src.size())\n",
    "# pos_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b49b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output_trans.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a8cc1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50, 1024])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87ae68e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2560000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd12e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b62dc123",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87f1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a400a8f",
   "metadata": {},
   "source": [
    "# Get the my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a984e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2faba6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(data):\n",
    "    '''recale the data, which is a 2D matrix, to 0-1'''\n",
    "    return (data - data.min(axis=1)[:, np.newaxis]) / (data.max(axis=1) - data.min(axis=1))[:, np.newaxis]\n",
    "\n",
    "\n",
    "class DepressionDataset(Dataset):\n",
    "    '''create a training, develop, or test dataset\n",
    "       and load the participant features if it's called\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 mode,\n",
    "                 transform=None):\n",
    "        super(DepressionDataset, self).__init__()\n",
    "\n",
    "        # only train, develop, test dataset allow\n",
    "        assert mode in [\"train\", \"validation\", \"test\"], \\\n",
    "            \"Argument --mode could only be ['train', 'validation', 'test']\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train_data_path = os.path.join(self.root_dir, 'train_split_Depression_AVEC2017.csv')\n",
    "        self.valid_data_path = os.path.join(self.root_dir, 'dev_split_Depression_AVEC2017.csv')\n",
    "        self.test_data_path = os.path.join(self.root_dir, 'full_test_split.csv')\n",
    "        # load sent2vec model for converting text file to 2D array\n",
    "        self.sent2vec = SentenceTransformer('all-mpnet-base-v2')  # output dimension 768\n",
    "\n",
    "        # load training data # 107 sessions\n",
    "        if self.mode == \"train\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.train_data_path))\n",
    "            # store ground truth\n",
    "            ####################################################################################################\n",
    "            # self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.patientIDs = np.array([303, 321, 362, 363, 426])  # for debugging on my laptop\n",
    "            ####################################################################################################\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load development data # 35 sessions\n",
    "        if self.mode == \"validation\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.valid_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load test data # 47 sessions\n",
    "        if self.mode == \"test\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.test_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = None\n",
    "\n",
    "    def pre_check(self, data):\n",
    "        '''\n",
    "        Basic cleaning process to make sure no missing value\n",
    "        and that the sum of each PHQ subscore equals to PHQ score\n",
    "        Argument:\n",
    "            data: numpy array\n",
    "        Return:\n",
    "            data: numpy array with type \"int\"\n",
    "        '''\n",
    "        # make sure no NaN, Inf, -Inf\n",
    "        if data.isin([np.nan, np.inf, -np.inf]).any(1).sum():\n",
    "            print('Replacing NaN, Inf, or -Inf ...')\n",
    "            data = data.replace([np.inf, -np.inf, np.nan], 0)  # .astype('int')\n",
    "        else:\n",
    "            data = data  # .astype('int')\n",
    "\n",
    "        # compare the sum of each PHQ subscore to PHQ score\n",
    "        unequal = data.iloc[:, 4:].sum(axis=1) != data.iloc[:, 2]\n",
    "        if unequal.any() and self.mode != 'test':\n",
    "            lines = np.where(unequal)\n",
    "            raise ValueError((\"The sum of each PHQ subscore at line {} \"\n",
    "                              \"is unequal to the PHQ score\").format(lines[0]))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patientIDs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.patientIDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Essentional function for creating dataset in PyTorch, which will automatically be\n",
    "        called in Dataloader and load all the extracted features of the patient in the Batch\n",
    "        based on the index of self.patientIDs\n",
    "        Argument:\n",
    "            idx: int, index of the patient ID in self.patientIDs\n",
    "        Return:\n",
    "            session: dict, contains all the extracted features and ground truth of a patient/session\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # get the patient session path\n",
    "        session_num = self.patientIDs[idx]\n",
    "        session_path = os.path.join(self.root_dir, '{}_P'.format(session_num))\n",
    "\n",
    "        # TODO: if other feature is needed, add more in the following part...\n",
    "\n",
    "        # get text path\n",
    "        text_path = os.path.join(session_path, '{}_TRANSCRIPT.csv'.format(session_num))\n",
    "\n",
    "        # text feature\n",
    "        self.text_feature = self.load_sent2vec(text_path, speaker='Participant')\n",
    "        sentence_embedding = self.text_feature['sentence_embeddings']\n",
    "\n",
    "        # summary\n",
    "        session = {'patientID': session_num,\n",
    "                   'session_path': session_path,\n",
    "                   'sentence_embeddings': sentence_embedding,\n",
    "                   'phq_score_gt': self.phq_score_gt[idx],\n",
    "                   'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "                   'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "                   'gender_gt': self.gender_gt[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            session = self.transform(session)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def load_sent2vec(self, text_path, speaker='Participant'):\n",
    "        '''\n",
    "        load the text file and use sent2vec model from SentenceTransformer\n",
    "        for sentence embeddings, which generates 2D array\n",
    "        Arguments:\n",
    "            text_path: string, absolute path to transcipt file\n",
    "            speaker: certain string, which transcript of the speaker to load\n",
    "        Return:\n",
    "            text_feature: dict, contain converted embedding vectors, sentences, etc.\n",
    "        '''\n",
    "\n",
    "        # only 'Ellie', 'Participant', 'both' are allow\n",
    "        assert speaker in ['Ellie', 'Participant', 'both'], \\\n",
    "            \"Argument --speaker could only be ['Ellie', 'Participant', 'both']\"\n",
    "\n",
    "        text_file = pd.read_csv(text_path)\n",
    "        # tokenize the text file, filter out all \\t space and unnecessary columns such as time, participent\n",
    "        tokenized_words = self.tokenize_corpus(text_file.values.tolist()[i][0] for i in range(text_file.shape[0]))\n",
    "\n",
    "        sentences = []\n",
    "        sentences_idx = []\n",
    "\n",
    "        if speaker == 'Ellie':\n",
    "            for idx, sentence in enumerate(tokenized_words):\n",
    "                if sentence[2] == 'Ellie':\n",
    "                    sentences.append(sentence[3:])\n",
    "                    sentences_idx.append(idx)\n",
    "        elif speaker == 'Participant':\n",
    "            for idx, sentence in enumerate(tokenized_words):\n",
    "                if sentence[2] == 'Participant':\n",
    "                    sentences.append(sentence[3:])\n",
    "                    sentences_idx.append(idx)\n",
    "\n",
    "        else:  # speaker == 'both'\n",
    "            sentences = [tokenized_words[i][3:] for i in range(len(tokenized_words))]\n",
    "            sentences_idx = list(range(len(tokenized_words)))\n",
    "\n",
    "        # recombine 2D list of words into 1D list of sentence\n",
    "        final_sentences = [\" \".join(sentences[i]).lower() for i in range(len(sentences))]\n",
    "        # convert sentence to vector with SentenceTransformer pretrained model\n",
    "        sentence_embeddings = self.sent2vec.encode(final_sentences)\n",
    "        h, w = sentence_embeddings.shape  # num_sentences x 768\n",
    "        sentence_embeddings = minmax_scaler(sentence_embeddings).reshape(h, int(w/3), 3)  # num_sentences x 256 x 3\n",
    "        # summary\n",
    "        text_feature = {'speaker': speaker,\n",
    "                        'sentence_embeddings': sentence_embeddings,\n",
    "                        'sentences': final_sentences,\n",
    "                        'indices': sentences_idx}\n",
    "\n",
    "        return text_feature\n",
    "\n",
    "    def tokenize_corpus(self, corpus):\n",
    "        '''tokenzie a given list of string into list of words\n",
    "        Argument:\n",
    "            corpus: 1D list of string, each element is a sting of sentence\n",
    "        Return:\n",
    "            tokens: 2D list of string, each raw is a list of words splitted from sentence\n",
    "        '''\n",
    "        tokens = [x.split() for x in corpus]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class Padding(object):\n",
    "    ''' pad zero to each feature matrix so that they all have the same size '''\n",
    "\n",
    "    def __init__(self, text_output_size=(386, 768)):\n",
    "        super(Padding, self).__init__()\n",
    "        '''\n",
    "        Each output size could be 'int' or 'tuple'. \n",
    "        Integer would be the number of desired rows\n",
    "        and Tuple would be the desired 2D array size.\n",
    "\n",
    "        Here is recommended to keep the number of columns \n",
    "        as they are and only set the number of rows with int\n",
    "\n",
    "        To find the maximum length of rows, please use the \n",
    "        'find_max_length' function in utils to search through. \n",
    "\n",
    "        The value 386 are the maximum length in our case.\n",
    "        '''\n",
    "        assert isinstance(text_output_size, (int, tuple))\n",
    "        self.text_output_size = text_output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings']\n",
    "\n",
    "        # text padding\n",
    "        if isinstance(self.text_output_size, int):\n",
    "            shape = sentence_embeddings.shape\n",
    "            assert self.text_output_size >= shape[0], \\\n",
    "                \"audio output size should be bigger than {}\".format(shape[0])\n",
    "            padded_text = np.zeros((self.text_output_size, shape[1]))\n",
    "            padded_text[:shape[0], :shape[1]] = sentence_embeddings\n",
    "        else:\n",
    "            shape = sentence_embeddings.shape\n",
    "            assert self.text_output_size[0] >= shape[0] and self.text_output_size[1] >= shape[1], \\\n",
    "                \"text output size should be bigger than {}\".format(shape)\n",
    "            padded_text = np.zeros(self.text_output_size)\n",
    "            padded_text[:shape[0], :shape[1]] = sentence_embeddings\n",
    "\n",
    "        # summary\n",
    "        padded_session = {'patientID': session['patientID'],\n",
    "                          'session_path': session['session_path'],\n",
    "                          'sentence_embeddings': padded_text,\n",
    "                          'phq_score_gt': session['phq_score_gt'],\n",
    "                          'phq_binay_gt': session['phq_binay_gt'],\n",
    "                          'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                          'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return padded_session\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int),  Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size=(256, 256)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings']\n",
    "\n",
    "        h, w = sentence_embeddings.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        rescaled_sentence_embeddings = transform.resize(sentence_embeddings, (new_h, new_w))\n",
    "\n",
    "        # summary\n",
    "        rescaled_session = {'patientID': session['patientID'],\n",
    "                            'session_path': session['session_path'],\n",
    "                            'sentence_embeddings': rescaled_sentence_embeddings,\n",
    "                            'phq_score_gt': session['phq_score_gt'],\n",
    "                            'phq_binay_gt': session['phq_binay_gt'],\n",
    "                            'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                            'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return rescaled_session\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int), Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size=(224, 224)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings']\n",
    "\n",
    "        h, w = sentence_embeddings.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        cropped_sentence_embeddings = sentence_embeddings[top:top + new_h, left:left + new_w]\n",
    "\n",
    "        # summary\n",
    "        cropped_session = {'patientID': session['patientID'],\n",
    "                           'session_path': session['session_path'],\n",
    "                           'sentence_embeddings': cropped_sentence_embeddings,\n",
    "                           'phq_score_gt': session['phq_score_gt'],\n",
    "                           'phq_binay_gt': session['phq_binay_gt'],\n",
    "                           'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                           'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return cropped_session\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors or np.int to torch.tensor.\"\"\"\n",
    "\n",
    "    def __call__(self, session):\n",
    "        converted_session = {'patientID': session['patientID'],\n",
    "                             'session_path': session['session_path'],\n",
    "                             'sentence_embeddings': torch.from_numpy(session['sentence_embeddings']).type(\n",
    "                                 torch.FloatTensor),\n",
    "                             'phq_score_gt': torch.tensor(session['phq_score_gt']).type(torch.FloatTensor),\n",
    "                             'phq_binay_gt': torch.tensor(session['phq_binay_gt']).type(torch.FloatTensor),\n",
    "                             'phq_subscores_gt': torch.from_numpy(session['phq_subscores_gt']).type(torch.FloatTensor),\n",
    "                             'gender_gt': torch.tensor(session['gender_gt']).type(torch.FloatTensor)}\n",
    "\n",
    "        return converted_session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b3c8d",
   "metadata": {},
   "source": [
    "# Get DataLoader and see how it is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e949bf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number:  0 , sentence embeddings:  torch.Size([2, 224, 224, 3])\n",
      "{'patientID': tensor([303, 321], dtype=torch.int32), 'session_path': ['C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\303_P', 'C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\321_P'], 'sentence_embeddings': tensor([[[[0.4850, 0.6227, 0.5460],\n",
      "          [0.5285, 0.7611, 0.5326],\n",
      "          [0.5800, 0.5206, 0.4511],\n",
      "          ...,\n",
      "          [0.5150, 0.5077, 0.5798],\n",
      "          [0.5653, 0.4846, 0.2573],\n",
      "          [0.4302, 0.3795, 0.4945]],\n",
      "\n",
      "         [[0.5560, 0.5559, 0.4355],\n",
      "          [0.5785, 0.8480, 0.4720],\n",
      "          [0.4077, 0.5092, 0.5353],\n",
      "          ...,\n",
      "          [0.5497, 0.4464, 0.5438],\n",
      "          [0.5685, 0.5042, 0.2923],\n",
      "          [0.4463, 0.4576, 0.4820]],\n",
      "\n",
      "         [[0.5627, 0.5251, 0.3599],\n",
      "          [0.5938, 0.8525, 0.4618],\n",
      "          [0.3178, 0.5030, 0.5507],\n",
      "          ...,\n",
      "          [0.5550, 0.4326, 0.5113],\n",
      "          [0.5889, 0.4786, 0.3421],\n",
      "          [0.4357, 0.4690, 0.4952]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5488, 0.5399, 0.6687],\n",
      "          [0.3910, 0.7258, 0.5506],\n",
      "          [0.4868, 0.3767, 0.6512],\n",
      "          ...,\n",
      "          [0.4865, 0.6790, 0.3951],\n",
      "          [0.6899, 0.4620, 0.1763],\n",
      "          [0.6550, 0.5973, 0.4999]],\n",
      "\n",
      "         [[0.5464, 0.5491, 0.6992],\n",
      "          [0.5418, 0.5069, 0.5998],\n",
      "          [0.5668, 0.3431, 0.6731],\n",
      "          ...,\n",
      "          [0.3800, 0.6654, 0.3079],\n",
      "          [0.6422, 0.4082, 0.4452],\n",
      "          [0.4172, 0.5069, 0.5240]],\n",
      "\n",
      "         [[0.5546, 0.5486, 0.7143],\n",
      "          [0.6459, 0.3335, 0.6434],\n",
      "          [0.6513, 0.2993, 0.6679],\n",
      "          ...,\n",
      "          [0.2949, 0.6456, 0.2438],\n",
      "          [0.6172, 0.3572, 0.6704],\n",
      "          [0.2211, 0.4207, 0.5210]]],\n",
      "\n",
      "\n",
      "        [[[0.6297, 0.6422, 0.3249],\n",
      "          [0.2910, 0.6760, 0.1288],\n",
      "          [0.5572, 0.6115, 0.7074],\n",
      "          ...,\n",
      "          [0.5089, 0.6365, 0.5439],\n",
      "          [0.4882, 0.3958, 0.5687],\n",
      "          [0.6130, 0.6975, 0.4168]],\n",
      "\n",
      "         [[0.6839, 0.6526, 0.2861],\n",
      "          [0.3975, 0.6732, 0.2904],\n",
      "          [0.4388, 0.5949, 0.5833],\n",
      "          ...,\n",
      "          [0.4789, 0.5390, 0.5305],\n",
      "          [0.5362, 0.3601, 0.5204],\n",
      "          [0.5951, 0.6511, 0.4805]],\n",
      "\n",
      "         [[0.7410, 0.6510, 0.2604],\n",
      "          [0.5369, 0.6611, 0.5245],\n",
      "          [0.2851, 0.5977, 0.4297],\n",
      "          ...,\n",
      "          [0.4613, 0.4130, 0.5093],\n",
      "          [0.6013, 0.3329, 0.4617],\n",
      "          [0.5829, 0.5741, 0.5573]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5726, 0.7254, 0.5152],\n",
      "          [0.7732, 0.6653, 0.7026],\n",
      "          [0.3175, 0.6406, 0.4517],\n",
      "          ...,\n",
      "          [0.5047, 0.5954, 0.5629],\n",
      "          [0.3845, 0.5115, 0.2033],\n",
      "          [0.4483, 0.5174, 0.5103]],\n",
      "\n",
      "         [[0.6512, 0.6727, 0.6583],\n",
      "          [0.4447, 0.6472, 0.7484],\n",
      "          [0.5061, 0.6425, 0.4972],\n",
      "          ...,\n",
      "          [0.6700, 0.6985, 0.5106],\n",
      "          [0.5271, 0.6410, 0.3648],\n",
      "          [0.4779, 0.4690, 0.4559]],\n",
      "\n",
      "         [[0.6627, 0.5863, 0.6774],\n",
      "          [0.2789, 0.6357, 0.7751],\n",
      "          [0.6485, 0.6396, 0.5332],\n",
      "          ...,\n",
      "          [0.7575, 0.7541, 0.4817],\n",
      "          [0.5927, 0.7139, 0.5001],\n",
      "          [0.4984, 0.4219, 0.4080]]]]), 'phq_score_gt': tensor([0., 6.]), 'phq_binay_gt': tensor([0., 0.]), 'phq_subscores_gt': tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 2., 2., 0., 0., 0.]]), 'gender_gt': tensor([0., 0.])}\n",
      "=================================\n",
      "Batch number:  1 , sentence embeddings:  torch.Size([2, 224, 224, 3])\n",
      "{'patientID': tensor([362, 363], dtype=torch.int32), 'session_path': ['C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\362_P', 'C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\363_P'], 'sentence_embeddings': tensor([[[[0.4273, 0.7463, 0.4486],\n",
      "          [0.5049, 0.7975, 0.2258],\n",
      "          [0.6005, 0.4482, 0.4896],\n",
      "          ...,\n",
      "          [0.5225, 0.4218, 0.6281],\n",
      "          [0.4072, 0.6169, 0.7022],\n",
      "          [0.5344, 0.2880, 0.6784]],\n",
      "\n",
      "         [[0.3900, 0.6980, 0.5040],\n",
      "          [0.4841, 0.7682, 0.2837],\n",
      "          [0.5224, 0.5117, 0.4402],\n",
      "          ...,\n",
      "          [0.5080, 0.4347, 0.5094],\n",
      "          [0.4376, 0.5301, 0.6981],\n",
      "          [0.5437, 0.4039, 0.6263]],\n",
      "\n",
      "         [[0.3528, 0.6497, 0.5593],\n",
      "          [0.4632, 0.7388, 0.3415],\n",
      "          [0.4443, 0.5753, 0.3907],\n",
      "          ...,\n",
      "          [0.4935, 0.4475, 0.3908],\n",
      "          [0.4681, 0.4434, 0.6940],\n",
      "          [0.5529, 0.5197, 0.5742]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5823, 0.6152, 0.5378],\n",
      "          [0.6179, 0.5817, 0.8135],\n",
      "          [0.5994, 0.6887, 0.4955],\n",
      "          ...,\n",
      "          [0.3863, 0.5470, 0.6384],\n",
      "          [0.5345, 0.6268, 0.5937],\n",
      "          [0.6361, 0.6027, 0.6286]],\n",
      "\n",
      "         [[0.6089, 0.5711, 0.5668],\n",
      "          [0.5256, 0.5637, 0.7998],\n",
      "          [0.6767, 0.6441, 0.5341],\n",
      "          ...,\n",
      "          [0.3974, 0.5812, 0.7177],\n",
      "          [0.4617, 0.6263, 0.5856],\n",
      "          [0.6166, 0.6490, 0.7045]],\n",
      "\n",
      "         [[0.6224, 0.5310, 0.5911],\n",
      "          [0.4441, 0.5429, 0.7951],\n",
      "          [0.7423, 0.6032, 0.5638],\n",
      "          ...,\n",
      "          [0.4071, 0.6062, 0.7750],\n",
      "          [0.3892, 0.6255, 0.5728],\n",
      "          [0.5962, 0.6848, 0.7605]]],\n",
      "\n",
      "\n",
      "        [[[0.5893, 0.4918, 0.6059],\n",
      "          [0.7496, 0.6451, 0.4707],\n",
      "          [0.6488, 0.4515, 0.5104],\n",
      "          ...,\n",
      "          [0.5735, 0.6722, 0.6032],\n",
      "          [0.4563, 0.3975, 0.6439],\n",
      "          [0.5463, 0.7287, 0.5267]],\n",
      "\n",
      "         [[0.5136, 0.3339, 0.5695],\n",
      "          [0.5746, 0.6558, 0.4659],\n",
      "          [0.5820, 0.4939, 0.4325],\n",
      "          ...,\n",
      "          [0.5548, 0.6012, 0.5087],\n",
      "          [0.6072, 0.3994, 0.5769],\n",
      "          [0.5197, 0.6288, 0.6290]],\n",
      "\n",
      "         [[0.4304, 0.1880, 0.5145],\n",
      "          [0.4273, 0.6143, 0.4601],\n",
      "          [0.4367, 0.5408, 0.3042],\n",
      "          ...,\n",
      "          [0.5243, 0.5167, 0.3971],\n",
      "          [0.7729, 0.3864, 0.4818],\n",
      "          [0.4929, 0.5354, 0.7024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5109, 0.4325, 0.5117],\n",
      "          [0.3354, 0.4401, 0.5926],\n",
      "          [0.7163, 0.3485, 0.5879],\n",
      "          ...,\n",
      "          [0.4788, 0.4783, 0.6396],\n",
      "          [0.9166, 0.5607, 0.5353],\n",
      "          [0.5641, 0.6782, 0.3795]],\n",
      "\n",
      "         [[0.4624, 0.3565, 0.6579],\n",
      "          [0.2855, 0.4013, 0.4171],\n",
      "          [0.5946, 0.3627, 0.5392],\n",
      "          ...,\n",
      "          [0.3096, 0.4547, 0.6045],\n",
      "          [0.7928, 0.6958, 0.4495],\n",
      "          [0.5291, 0.7807, 0.3222]],\n",
      "\n",
      "         [[0.5683, 0.4355, 0.4108],\n",
      "          [0.4318, 0.4936, 0.5637],\n",
      "          [0.4784, 0.2077, 0.6975],\n",
      "          ...,\n",
      "          [0.4156, 0.5364, 0.7346],\n",
      "          [0.7253, 0.5748, 0.6408],\n",
      "          [0.4096, 0.7923, 0.3260]]]]), 'phq_score_gt': tensor([7., 4.]), 'phq_binay_gt': tensor([0., 0.]), 'phq_subscores_gt': tensor([[0., 1., 1., 2., 2., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0.]]), 'gender_gt': tensor([1., 1.])}\n",
      "=================================\n",
      "Batch number:  2 , sentence embeddings:  torch.Size([1, 224, 224, 3])\n",
      "{'patientID': tensor([426], dtype=torch.int32), 'session_path': ['C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\426_P'], 'sentence_embeddings': tensor([[[[0.6541, 0.6616, 0.6107],\n",
      "          [0.4389, 0.5666, 0.5401],\n",
      "          [0.5643, 0.4712, 0.5156],\n",
      "          ...,\n",
      "          [0.5791, 0.5539, 0.6641],\n",
      "          [0.5370, 0.4186, 0.4544],\n",
      "          [0.4610, 0.4623, 0.7419]],\n",
      "\n",
      "         [[0.7037, 0.3840, 0.6029],\n",
      "          [0.5947, 0.3710, 0.4390],\n",
      "          [0.3658, 0.4238, 0.5772],\n",
      "          ...,\n",
      "          [0.3354, 0.6030, 0.4392],\n",
      "          [0.5322, 0.5048, 0.3826],\n",
      "          [0.5389, 0.4168, 0.7690]],\n",
      "\n",
      "         [[0.5925, 0.5430, 0.5957],\n",
      "          [0.8818, 0.7912, 0.5989],\n",
      "          [0.4828, 0.5011, 0.7137],\n",
      "          ...,\n",
      "          [0.0912, 0.4788, 0.5974],\n",
      "          [0.3259, 0.6810, 0.4265],\n",
      "          [0.5638, 0.3850, 0.7592]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.3794, 0.5893, 0.7395],\n",
      "          [0.5975, 0.5180, 0.4675],\n",
      "          [0.5319, 0.3543, 0.5250],\n",
      "          ...,\n",
      "          [0.4278, 0.5351, 0.4674],\n",
      "          [0.5347, 0.5974, 0.4773],\n",
      "          [0.5563, 0.3880, 0.5884]],\n",
      "\n",
      "         [[0.5196, 0.5740, 0.5325],\n",
      "          [0.5898, 0.5761, 0.4799],\n",
      "          [0.4701, 0.3495, 0.3354],\n",
      "          ...,\n",
      "          [0.3613, 0.5520, 0.4083],\n",
      "          [0.5233, 0.6643, 0.4083],\n",
      "          [0.5297, 0.3277, 0.7766]],\n",
      "\n",
      "         [[0.9009, 0.3310, 0.4835],\n",
      "          [0.4687, 0.6018, 0.4323],\n",
      "          [0.4219, 0.5571, 0.4626],\n",
      "          ...,\n",
      "          [0.2910, 0.6339, 0.3681],\n",
      "          [0.7101, 0.4523, 0.4666],\n",
      "          [0.5272, 0.5245, 0.6456]]]]), 'phq_score_gt': tensor([2.]), 'phq_binay_gt': tensor([0.]), 'phq_subscores_gt': tensor([[0., 0., 1., 1., 0., 0., 0., 0.]]), 'gender_gt': tensor([1.])}\n",
      "=================================\n",
      "number of the batches in DataLoader is 3\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# test 3: try to load the dataset with DataLoader\n",
    "transformed_dataset = DepressionDataset(os.path.join(os.getcwd(), 'DAIC-WOZ Dataset'), 'train',\n",
    "                                        transform=transforms.Compose([Rescale((256, 256)),\n",
    "                                                                      RandomCrop((224, 224)),\n",
    "                                                                      ToTensor()]))\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset, \n",
    "                        batch_size=2,\n",
    "                        shuffle=False, \n",
    "                        num_workers=0)\n",
    "# iterate through batches\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print('Batch number: ', i_batch, ', sentence embeddings: ', sample_batched['sentence_embeddings'].size())\n",
    "    print(sample_batched)\n",
    "    print('=================================')\n",
    "    \n",
    "    if i_batch == 0:\n",
    "        sample = sample_batched['sentence_embeddings']\n",
    "    \n",
    "print('number of the batches in DataLoader is {}'.format(len(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ef8905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 224, 224, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f997e2",
   "metadata": {},
   "source": [
    "####  use 'permute' to make sure the channel dimension is in 2 position to match the input requirement format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ad2d5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.permute(0, 3, 1, 2).contiguous().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4fca0",
   "metadata": {},
   "source": [
    "##### remember to convert to torch.FloatTensor in order to fit in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683ccada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0131,  0.0572, -0.0360,  ..., -0.0415, -0.0471,  0.0127],\n",
       "        [-0.0290,  0.0563, -0.0076,  ..., -0.0085, -0.0270, -0.0027],\n",
       "        [ 0.0337, -0.0226, -0.0394,  ...,  0.0040, -0.0337, -0.0060],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset[0]['sentence_embeddings']  # .type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2613f23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset[0]['sentence_embeddings'].type(torch.FloatTensor).shape  # .type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785db797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdea2908",
   "metadata": {},
   "source": [
    "### How list.extend() and torchElement.item() and torchVector.numpy() works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2c119ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list = []\n",
    "torch.tensor([2,3]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445f31b",
   "metadata": {},
   "source": [
    "##### 2 ways to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "a11e08a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list.extend(torch.tensor([2,3]).numpy())\n",
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "ab3d4e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list.extend([i.item() for i in torch.tensor([2,3])])\n",
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a717132c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_list = []\n",
    "exp2 = torch.tensor([[2],[3]])\n",
    "exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "9a651d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_list.extend([i.item() for i in exp2])\n",
    "b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "fcf72944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2], dtype=int64), array([3], dtype=int64)]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_list.extend(exp2.numpy())\n",
    "b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee70e9e",
   "metadata": {},
   "source": [
    "### How to extend a torch tensor to a single list of float or list of numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "251211ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2, 2, 1, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = torch.tensor([[0, 1, 1, 2, 2, 1, 0, 0],\n",
    "                    [1, 1, 0, 0, 0, 1, 1, 0]])\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c5f0e2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 2 1 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "[1 1 0 0 0 1 1 0]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for i in exp:\n",
    "    print(i.numpy())\n",
    "    print(type(i.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28c6dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in exp:\n",
    "    a_list.extend(i.numpy().astype(float))\n",
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "acea9d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 1., 2., 2., 1., 0., 0.]),\n",
       " array([1., 1., 0., 0., 0., 1., 1., 0.])]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list.extend([exp[i].numpy().astype(float) for i in range(exp.size(0))])\n",
    "a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "69e5707e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 2., 2., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convet the whole list of numpy arrays to numpy arrays\n",
    "np.array(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd7635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bced33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e3e311",
   "metadata": {},
   "source": [
    "## Debug: How to convert a batch of 'phq_subscores_gt' to soft_subscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448cbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa7207d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patientID': 363,\n",
       " 'session_path': 'C:\\\\Users\\\\denni\\\\Documents\\\\KIT Studium\\\\Bachelorarbeit\\\\DAIC-WOZ Dataset\\\\363_P',\n",
       " 'sentence_embeddings': tensor([[-0.0550, -0.0017, -0.0115,  ...,  0.0490, -0.0064, -0.0252],\n",
       "         [ 0.0420, -0.0161, -0.0109,  ...,  0.0055, -0.0070,  0.0034],\n",
       "         [ 0.0169,  0.0313, -0.0218,  ...,  0.0108, -0.0282, -0.0090],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " 'phq_score_gt': tensor(4),\n",
       " 'phq_binay_gt': tensor(0),\n",
       " 'phq_subscores_gt': tensor([1, 1, 0, 0, 0, 1, 1, 0]),\n",
       " 'gender_gt': tensor(1)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cc33b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 0, 0, 0, 1, 1, 0]), tensor([0, 1, 1, 2, 2, 1, 0, 0]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get two torch tensors\n",
    "sub1 = transformed_dataset[3]['phq_subscores_gt']\n",
    "sub2 = transformed_dataset[2]['phq_subscores_gt']\n",
    "sub1,sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b18b7bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 2, 3, 1, 0]), tensor([0, 1, 1, 2, 2, 1, 0, 0]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = torch.tensor([0, 1, 2, 3, 2, 3, 1, 0])\n",
    "sub2 = torch.tensor([0, 1, 1, 2, 2, 1, 0, 0])\n",
    "\n",
    "sub1,sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96b5c625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 2, 3, 1, 0],\n",
       "        [0, 1, 1, 2, 2, 1, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test how torch.stack works\n",
    "torch.stack([sub1, sub2], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ece72",
   "metadata": {},
   "source": [
    "### - Soft_subscore version 1: resolution based on our class number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2305d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_soft_subscores(subscores, config):\n",
    "    # Each subscore choose a score from [0, 1, 2, 3], we normalize it into 0 ~ config['CLASSES_RESOLUTION']\n",
    "    tmp = [stats.norm.pdf(np.arange(config['CLASSES_RESOLUTION']),\n",
    "                          loc=subscore / ((config['N_CLASSES'] - 1) / (config['CLASSES_RESOLUTION']-1)),\n",
    "                          scale=config['STD']).astype(np.float32)\n",
    "           for subscore in subscores]\n",
    "    tmp = np.stack(tmp)\n",
    "    return torch.from_numpy(tmp / tmp.sum(axis=-1, keepdims=True))  # 8x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71fbb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soft_subscores(subscores_gt, config):\n",
    "\n",
    "    soft_subscores_gt = torch.tensor([[]])\n",
    "\n",
    "    for i in range(len(subscores_gt)):\n",
    "        subscores = subscores_gt[i]\n",
    "        converted_subscores = convert_soft_subscores(subscores, config).view(-1, config['N_SUBSCORES'],\n",
    "                                                                             config['CLASSES_RESOLUTION'])\n",
    "        if i == 0:\n",
    "            soft_subscores_gt = converted_subscores\n",
    "        else:\n",
    "            soft_subscores_gt = torch.cat([soft_subscores_gt, converted_subscores], dim=0)\n",
    "\n",
    "    return soft_subscores_gt.view(-1, config['N_SUBSCORES'], config['CLASSES_RESOLUTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f120f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to create a batch of phq_subscores_gt to simulate the code process\n",
    "data = {}\n",
    "data['phq_subscores_gt'] = torch.stack([sub1, sub2], dim=0)\n",
    "\n",
    "config = {}\n",
    "config['N_CLASSES'] = 4\n",
    "config['N_SUBSCORES'] = 8\n",
    "config['CLASSES_RESOLUTION'] = 4\n",
    "config['STD'] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a754aaa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 2, 3, 1, 0],\n",
       "        [0, 1, 1, 2, 2, 1, 0, 0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['phq_subscores_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eb617d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [5.8818e-04, 2.9240e-02, 3.0468e-01, 6.6549e-01],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [5.8818e-04, 2.9240e-02, 3.0468e-01, 6.6549e-01],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]],\n",
       "\n",
       "        [[6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for converting a batcg of patient's subscores to soft_subscores\n",
    "result1 = get_soft_subscores(data['phq_subscores_gt'], config)  \n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6da381e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the output shape is batch_size x number_of_subscores x classes_number\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f66e9bbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "        [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "        [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "        [5.8818e-04, 2.9240e-02, 3.0468e-01, 6.6549e-01],\n",
       "        [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "        [5.8818e-04, 2.9240e-02, 3.0468e-01, 6.6549e-01],\n",
       "        [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "        [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(result1[0].T)\n",
    "result1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9a01c",
   "metadata": {},
   "source": [
    "### - Soft_subscore version 2: resolution bigger than class number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6a2d0f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 28])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {}\n",
    "config['N_CLASSES'] = 4\n",
    "config['N_SUBSCORES'] = 8\n",
    "config['CLASSES_RESOLUTION'] = 28\n",
    "config['STD'] = 5\n",
    "\n",
    "result2 = get_soft_subscores(data['phq_subscores_gt'], config)  \n",
    "result2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4e5fef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26343216ca0>,\n",
       " <matplotlib.lines.Line2D at 0x26343216dc0>,\n",
       " <matplotlib.lines.Line2D at 0x26343216e80>,\n",
       " <matplotlib.lines.Line2D at 0x26343216f40>,\n",
       " <matplotlib.lines.Line2D at 0x26343216fa0>,\n",
       " <matplotlib.lines.Line2D at 0x26343226100>,\n",
       " <matplotlib.lines.Line2D at 0x263432261c0>,\n",
       " <matplotlib.lines.Line2D at 0x26343226280>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(result[0].T)\n",
    "plt.plot(result2[0].T)\n",
    "# result2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df35d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7052b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74b8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "169d28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.tensor([[2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]])\n",
    "\n",
    "tensor_b = torch.tensor([[6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
    "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]])\n",
    "\n",
    "tensor_c = torch.tensor([[2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
    "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75857b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]],\n",
       "\n",
       "        [[6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [2.2421e-02, 2.3364e-01, 5.1031e-01, 2.3364e-01],\n",
       "         [2.3364e-01, 5.1031e-01, 2.3364e-01, 2.2421e-02],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04],\n",
       "         [6.6549e-01, 3.0468e-01, 2.9240e-02, 5.8818e-04]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([tensor_a, tensor_b], dim=0).view(-1, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a55570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b2b47",
   "metadata": {},
   "source": [
    "### MUSDL Benchmark: result of converting the label to soft_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2773f0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  1.0000,  2.5000,  4.0000,  6.5000,  7.5000,  9.0000, 10.0000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([0, 1, 2.5,  4.,  6.5, 7.5,  9, 10])\n",
    "tensor\n",
    "\n",
    "# example subscores from dataset\n",
    "\n",
    "# [7.5 7.5 7.  7.  7.5 7.  7.5]\n",
    "# [7.5 7.  7.5 7.  7.5 7.  7. ]\n",
    "# [7.5 8.5 7.5 8.5 7.5 7.5 8. ]\n",
    "# [6.5 6.5 6.5 6.5 7.  6.5 6.5]\n",
    "# [6.  6.5 6.  6.5 6.  6.  6. ]\n",
    "# [7.5 7.  7.  7.  7.  6.5 7. ]\n",
    "# [9.  9.  9.  8.5 9.5 9.  9. ]\n",
    "# [7.5 7.5 7.5 7.  8.  7.5 7.5]\n",
    "# [7.5 7.  7.  7.  7.5 7.5 7. ]\n",
    "# [8.5 8.5 8.5 8.5 8.5 8.5 8. ]\n",
    "# [5.5 5.5 4.5 5.5 6.5 5.5 6. ]\n",
    "# [8.5 9.  8.5 8.5 9.  8.5 9. ]\n",
    "# [8.  8.  8.  8.5 8.  8.5 7.5]\n",
    "# [7.  7.  6.5 7.5 6.5 7.5 7. ]\n",
    "# [8.  8.  7.5 7.5 7.  7.5 7. ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d70d5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_label(tensor):\n",
    "    # Each judge choose a score from [0, 0.5, ..., 9.5, 10], we normalize it into 0~20\n",
    "    tmp = [stats.norm.pdf(np.arange(21), loc=judge_score * (21-1) / 10, scale=5).astype(np.float32)\n",
    "            for judge_score in tensor]\n",
    "    tmp = np.stack(tmp)\n",
    "    result = tmp / tmp.sum(axis=-1, keepdims=True)  # 7x21\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9cb8e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 21)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = proc_label(tensor)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14b788a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26342f508b0>,\n",
       " <matplotlib.lines.Line2D at 0x26342f50220>,\n",
       " <matplotlib.lines.Line2D at 0x26342f502e0>,\n",
       " <matplotlib.lines.Line2D at 0x26342f505e0>,\n",
       " <matplotlib.lines.Line2D at 0x26342f50850>,\n",
       " <matplotlib.lines.Line2D at 0x26342f35100>,\n",
       " <matplotlib.lines.Line2D at 0x26342f351c0>,\n",
       " <matplotlib.lines.Line2D at 0x26342f35280>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e0c0915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47790805e-01, 1.44864351e-01, 1.36428118e-01, 1.23445265e-01,\n",
       "       1.07318155e-01, 8.96396562e-02, 7.19375089e-02, 5.54675311e-02,\n",
       "       4.10913602e-02, 2.92476118e-02, 2.00013127e-02, 1.31417988e-02,\n",
       "       8.29620287e-03, 5.03190141e-03, 2.93233152e-03, 1.64180761e-03,\n",
       "       8.83201254e-04, 4.56483773e-04, 2.26683129e-04, 1.08153676e-04,\n",
       "       4.95782951e-05], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc79bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40a74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af223d79",
   "metadata": {},
   "source": [
    "# Test sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbe663bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time\\tstop_time\\tspeaker\\tvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.96\\t11.93\\tEllie\\thi i'm ellie thanks for co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.67\\t16.11\\tEllie\\ti was created to talk to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.9\\t23.37\\tEllie\\ti'm not a therapist but i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.93\\t28.64\\tEllie\\tand please feel free to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.16\\t30.47\\tEllie\\tare you okay with this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>533\\t534.1\\tParticipant\\ti like children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>535.1\\t535.8\\tParticipant\\tum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>539.15\\t541.41\\tEllie\\tokay i think i've asked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>542.15\\t543.63\\tEllie\\tthanks for sharing your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>544.47\\t545.04\\tEllie\\tgoodbye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start_time\\tstop_time\\tspeaker\\tvalue\n",
       "0    8.96\\t11.93\\tEllie\\thi i'm ellie thanks for co...\n",
       "1    12.67\\t16.11\\tEllie\\ti was created to talk to ...\n",
       "2    16.9\\t23.37\\tEllie\\ti'm not a therapist but i'...\n",
       "3    23.93\\t28.64\\tEllie\\tand please feel free to t...\n",
       "4         29.16\\t30.47\\tEllie\\tare you okay with this \n",
       "..                                                 ...\n",
       "124           533\\t534.1\\tParticipant\\ti like children\n",
       "125                     535.1\\t535.8\\tParticipant\\tum \n",
       "126  539.15\\t541.41\\tEllie\\tokay i think i've asked...\n",
       "127  542.15\\t543.63\\tEllie\\tthanks for sharing your...\n",
       "128                     544.47\\t545.04\\tEllie\\tgoodbye\n",
       "\n",
       "[129 rows x 1 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sent2vec = SentenceTransformer('all-mpnet-base-v2')\n",
    "# # start multi-processing\n",
    "# SentenceTransformer.start_multi_process_pool(target_devices: List[str] = None)\n",
    "\n",
    "text_path = os.path.join(os.getcwd(), 'DAIC-WOZ Dataset', '362_P', '362_TRANSCRIPT.csv')\n",
    "pd.read_csv(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0ee3f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(text_path, speaker='Participant'):\n",
    "    '''load transcript file and extract the text of the given speaker'''\n",
    "\n",
    "    def tokenize_corpus(corpus):\n",
    "        '''tokenzie a given list of string into list of words'''\n",
    "        tokens = [x.split() for x in corpus]\n",
    "        return tokens\n",
    "\n",
    "    # only 'Ellie', 'Participant', 'both' are allow\n",
    "    assert speaker in ['Ellie', 'Participant', 'both'], \\\n",
    "        \"Argument --speaker could only be ['Ellie', 'Participant', 'both']\"\n",
    "\n",
    "    text_file = pd.read_csv(text_path)\n",
    "    # tokenize the text file, filter out all \\t space and unnecessary columns such as time, participent\n",
    "    tokenized_words = tokenize_corpus(text_file.values.tolist()[i][0] for i in range(text_file.shape[0]))\n",
    "\n",
    "    sentences = []\n",
    "    sentences_idx = []\n",
    "\n",
    "    if speaker == 'Ellie':\n",
    "        for idx, sentence in enumerate(tokenized_words):\n",
    "            if sentence[2] == 'Ellie':\n",
    "                sentences.append(sentence[3:])\n",
    "                sentences_idx.append(idx)\n",
    "    elif speaker == 'Participant':\n",
    "        for idx, sentence in enumerate(tokenized_words):\n",
    "            if sentence[2] == 'Participant':\n",
    "                sentences.append(sentence[3:])\n",
    "                sentences_idx.append(idx)\n",
    "\n",
    "    else:  # speaker == 'both'\n",
    "        sentences = [tokenized_words[i][3:] for i in range(len(tokenized_words))]\n",
    "        sentences_idx = list(range(len(tokenized_words)))\n",
    "\n",
    "    # recombine 2D list of words into 1D list of sentence\n",
    "    final_sentences = [\" \".join(sentences[i]).lower() for i in range(len(sentences))]\n",
    "\n",
    "    return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3436caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentences = load_text_file(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c40305aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'good',\n",
       " 'from california',\n",
       " 'i like the eh arts and entertainments i like',\n",
       " 'the uh',\n",
       " 'you have a lot of freedom to work in a lot of different fields um',\n",
       " 'i like the fact that the weather is great um i like the fact that all my roots are here all my family is here',\n",
       " \"i don't like\",\n",
       " \"how hard it is to get places anymore it's very hard there's so much traffic it takes a long time to get where you're going um it's expensive it's a lot more expensive than it used to be\",\n",
       " 'not anymore',\n",
       " 'i studied journalism english music and special effects makeup',\n",
       " 'i am',\n",
       " 'now i am managing a doggy day care facility',\n",
       " 'my dream job is to',\n",
       " 'own my own company again um',\n",
       " 'probably working with animals',\n",
       " 'no',\n",
       " \"i'm i'm an artist i draw i paint i do tattoos\",\n",
       " 'i work a lot with animals',\n",
       " 'i like outdoor activities biking hiking',\n",
       " \"um people that aren't fair people that are\",\n",
       " 'angry and um people that are dishonest',\n",
       " 'usually i will give myself like a ten second timeout and think about it before i speak or react',\n",
       " \"i'm very good at controlling my temper\",\n",
       " 'probably my spouse um about something to do with our financial matters',\n",
       " \"i'm very close to my children and my father\",\n",
       " 'scrubbed_entry',\n",
       " \"no it's never easy it's the hardest job in the world but it's the best\",\n",
       " 'just um',\n",
       " \"wanting to prevent your children from making the mistakes you've had but you realize that they have to do that\",\n",
       " \"just to learn how to live life um that's one of the hardest things is letting them make their own mistakes\",\n",
       " 'the uh unconditional love my boys have for me the',\n",
       " \"everything that being a parent has made me a better person it's made me a better everything a better employee um i've\",\n",
       " 'i think being having my children made me',\n",
       " 'um much better person every day',\n",
       " \"probably i have a lot i've had a very full life\",\n",
       " \"i would be really hard to say to narrow that down i've had so many\",\n",
       " \"i'm a good friend i'm a true friend i'm honest i'm real\",\n",
       " \"i'm dependable\",\n",
       " \"and i don't play games i'm very i'm no <n> i'm no drama\",\n",
       " 'no',\n",
       " 'yes i have',\n",
       " 'in um',\n",
       " 'february of two thousand eleven',\n",
       " 'i was attacked by a stalker and almost killed in november of two thousand nine he broke into my apartment and laid in wait for me and attacked me when i came in the door and tried to kill me',\n",
       " 'i do',\n",
       " \"i think therapy has helped me tremendously i don't think i'd be able to hold a job and be responsible and\",\n",
       " 'be',\n",
       " 'a productive member of society <s> of society without therapy',\n",
       " \"i'm able to handle things better i don't get so um emotionally upset um a lot more rational i try to stay more rational than emotional more rational and logical\",\n",
       " \"so-so i've been sick and i've been uh run down a little stressed out\",\n",
       " 'i have yes i have',\n",
       " \"i use the resources <res> draw on the resources that i've learned and i reach out to other people i try not to isolate\",\n",
       " 'i try to um',\n",
       " 'use the tools that have been given to me in therapy um',\n",
       " \"i don't use medication anymore so it's mainly just proactive treatment where i have to uh do something about it\",\n",
       " \"it's not it's never easy it's always bad\",\n",
       " 'tired lethargic',\n",
       " \"um it's hard to keep my thoughts in order it's hard just to do the basics during the day\",\n",
       " 'mm no',\n",
       " \"i can't remember\",\n",
       " 'oh i went to the beach with my oldest son and his best friend and we rode three real three-wheel bikes and that was a lot of fun',\n",
       " \"it's hard to say right now i haven't decided i'm very proud of my children i would say that's probably it they're really good boys\",\n",
       " 'i have too many regrets right now',\n",
       " 'to finish school',\n",
       " 'to um',\n",
       " 'stay focused to not',\n",
       " 'let myself get distracted um',\n",
       " 'to focus more on long-term goals than short-term goals',\n",
       " 'my father',\n",
       " \"i'm very honest i'm very calm\",\n",
       " \"i don't get angry easily it takes a lot to provoke me\",\n",
       " \"i'm uh reliable if i say i'm gonna do something i do it\",\n",
       " \"and i'm uh i'm not i'm basically what you see is what you get i don't i don't pull any punches with people\",\n",
       " 'um anything to do with animals i love animals i always smile around animals any kind big small',\n",
       " 'um i like older people elderly people i like to help them and be around them',\n",
       " 'i like children',\n",
       " 'um']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "615c42e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8009e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c76acb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "32f5e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0, 2752,    2,  ...,    1,    1,    1],\n",
      "        [   0, 2208,    2,  ...,    1,    1,    1],\n",
      "        [   0, 2017, 2666,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0, 8533, 1049,  ...,    1,    1,    1],\n",
      "        [   0, 1049, 2070,  ...,    1,    1,    1],\n",
      "        [   0, 8533,    2,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(final_sentences, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "\n",
    "# #Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "# # print(model_output)\n",
    "\n",
    "# #Perform pooling. In this case, mean pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0e105047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 50])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b43eed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dec5f10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 768])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89549f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d9408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050533dc",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a52cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fe48a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_confusion_matrix(gt, pred):\n",
    "    \"\"\"\n",
    "    Make confusion matrix with format:\n",
    "                  -----------\n",
    "                  | TP | FP |\n",
    "                  -----------\n",
    "                  | FN | TN |\n",
    "                  -----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray - 1D\n",
    "    y_pred : ndarray - 1D\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray - 2D\n",
    "    \"\"\"\n",
    "    [[tn, fp], [fn, tp]] = confusion_matrix(np.asarray(gt), np.asarray(pred))\n",
    "    return np.array([[tp, fp], [fn, tn]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0b67456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = [1,0,1,0,1]\n",
    "pred = [1,1,0,1,1]\n",
    "standard_confusion_matrix(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62ccedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[tp, fp], [fn, tn]] = standard_confusion_matrix(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64bfa32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1, 0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9bd9d",
   "metadata": {},
   "source": [
    "# how the information is printed out in logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a25ab823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------ L + MUSDL ------------------------------------------------------\n",
      "Epoch: 0  Current Best: 0 at epoch 0\n",
      "  End of train:\n",
      "  | time: 1468.059s | LR:  0.05948 | Average Loss:  0.40988 | Accuracy: 80.00% (80/100) | train correlation: -0.408248 |\n",
      "  End of valid:\n",
      "  | time: 1468.059s | LR:  0.05948 | Average Loss:  0.40988 | Accuracy: 80.00% (80/100) | valid correlation: -0.408248 |\n",
      "\n",
      "  Output Scores:\n",
      "  - Classification:\n",
      "      TPR/Sensitivity:  0.56\n",
      "      TNR/Specificity:  0.56\n",
      "      Precision:  0.56\n",
      "      Recall:  0.56\n",
      "      F1-score:  0.56\n",
      "  - Regression:\n",
      "      MAE:  0.56\n",
      "      MSE:  0.56\n",
      "      RMSE:  0.56\n",
      "      R2:  0.56\n"
     ]
    }
   ],
   "source": [
    "print('-' * 54 + ' L + MUSDL ' + '-' * 54)\n",
    "print('Epoch: 0  Current Best: 0 at epoch 0')\n",
    "\n",
    "msg1 = '  End of {0}:\\n  | time: {1:8.3f}s | LR: {2:8.5f} | Average Loss: {3:8.5f} | Accuracy: {4:5.2f}% ({5}/{6}) | {7} correlation: {8:8.6f} |'.format('train', 1468.05947, 00.059478, 0.409878, 0.8*100, int(80), int(100), 'train', -0.408248290463863)\n",
    "print(msg1)\n",
    "\n",
    "msg2 = '  End of {0}:\\n  | time: {1:8.3f}s | LR: {2:8.5f} | Average Loss: {3:8.5f} | Accuracy: {4:5.2f}% ({5}/{6}) | {7} correlation: {8:8.6f} |'.format('valid', 1468.05947, 00.059478, 0.409878, 0.8*100, int(80), int(100), 'valid', -0.408248290463863)\n",
    "print(msg2)\n",
    "\n",
    "msg3 = ('\\n  Output Scores:')\n",
    "msg4 = ('  - Classification:\\n      TPR/Sensitivity: {0:5.2f}\\n      TNR/Specificity: {1:5.2f}\\n      Precision: {2:5.2f}\\n      Recall: {3:5.2f}\\n      F1-score: {4:5.2f}').format(0.55555,0.55555,0.55555,0.55555,0.55555)\n",
    "msg5 = ('  - Regression:\\n      MAE: {0:5.2f}\\n      MSE: {1:5.2f}\\n      RMSE: {2:5.2f}\\n      R2: {3:5.2f}').format(0.55555,0.55555,0.55555,0.55555)\n",
    "print(msg3)\n",
    "print(msg4)\n",
    "print(msg5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83f97a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "msg = ('  - Classification:\\n'\n",
    "       '      TPR/Sensitivity: {0:5.2f}\\n'\n",
    "       '      TNR/Specificity: {1:5.2f}\\n'\n",
    "       '      Precision: {2:5.2f}\\n'\n",
    "       '      Recall: {3:5.2f}\\n'\n",
    "       '      F1-score: {4:5.2f}').format(0.55555,0.55555,0.55555,0.55555,0.55555)\n",
    "\n",
    "print(type(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb507c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54544e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f4354a",
   "metadata": {},
   "source": [
    "# TensorBoard Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156e2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d39380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_train\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27201a8fcc32461c84b0c176dd3c355c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train\\MNIST\\raw\\train-images-idx3-ubyte.gz to mnist_train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_train\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cceb96b200c4dd39347599d61af09de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train\\MNIST\\raw\\train-labels-idx1-ubyte.gz to mnist_train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d16abdc9a943a2a01398dde758a3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to mnist_train\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b550197c6ee4d4d804fa84a9ef1238f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_train\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to mnist_train\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "model = torchvision.models.resnet50(False)\n",
    "# Have ResNet model take in grayscale rather than RGB\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('images', grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203af546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following code in cmd\n",
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd5a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# add_scala\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f87fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr_curve\n",
    "\n",
    "labels = np.random.randint(2, size=100)  # binary label\n",
    "predictions = np.random.rand(100)\n",
    "writer = SummaryWriter()\n",
    "writer.add_pr_curve('pr_curve', labels, predictions, 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "423f9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_scalars\n",
    "\n",
    "writer = SummaryWriter()\n",
    "r = 5\n",
    "for i in range(100):\n",
    "    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n",
    "                                    'xcosx':i*np.cos(i/r),\n",
    "                                    'tanx': np.tan(i/r)}, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97925955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973cb2f6",
   "metadata": {},
   "source": [
    "# Debug ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "48e1ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit import ViT\n",
    "\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f86ccc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale image\n",
    "\n",
    "def rescale(image, output_size):\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    if isinstance(output_size, int):\n",
    "        if h > w:\n",
    "            new_h, new_w = output_size * h / w, output_size\n",
    "        else:\n",
    "            new_h, new_w = output_size, output_size * w / h\n",
    "    else:\n",
    "        new_h, new_w = output_size\n",
    "\n",
    "    new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "    img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4234af00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = transformed_dataset[0]['sentence_embeddings']\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "56877c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 256, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.01223349, -0.00911181,  0.00822419, ...,  0.00410442,\n",
       "          -0.02116271, -0.02657881],\n",
       "         [ 0.01152719, -0.00885142,  0.00276996, ...,  0.00561038,\n",
       "          -0.02334277, -0.02871415],\n",
       "         [ 0.01291263, -0.00936219,  0.01346865, ...,  0.00265638,\n",
       "          -0.01906649, -0.02452561],\n",
       "         ...,\n",
       "         [-0.02238797,  0.02008174,  0.00525289, ...,  0.0149739 ,\n",
       "           0.00396261, -0.001725  ],\n",
       "         [-0.02557177,  0.02132559,  0.0069953 , ...,  0.0268412 ,\n",
       "          -0.00370663,  0.00898176],\n",
       "         [-0.02394866,  0.02069147,  0.00610701, ...,  0.02079121,\n",
       "           0.00020318,  0.00352341]]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = rescale(image[:102], (256, 256)).reshape((1,1,256, 256))\n",
    "print(img.shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a5776ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(image_size=256, patch_size=8, num_classes=1024, channels=1,\n",
    "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
    "model.train()\n",
    "output = model(torch.from_numpy(img).type(torch.FloatTensor))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ca150df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256, 256])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = rescale(image[:102], (256, 256)).reshape((1,256,256))\n",
    "img2 = rescale(image, (256, 256)).reshape((1,256,256))\n",
    "\n",
    "img = torch.stack([torch.from_numpy(img1), torch.from_numpy(img2)], dim=0).type(torch.FloatTensor)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "80a98236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(img)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "78f47985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(image_size=256, patch_size=8, num_classes=512, channels=1,\n",
    "            dim=256, depth=6, heads=8, mlp_dim=512)\n",
    "model.train()\n",
    "output = model(img)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24269e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViT(\n",
    "#     dim=128,\n",
    "#     image_size=224,\n",
    "#     patch_size=32,\n",
    "#     num_classes=2,\n",
    "#     transformer=efficient_transformer,\n",
    "#     channels=3,\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f37189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "eb3e38c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random crop the image\n",
    "\n",
    "image = transformed_dataset[0]['sentence_embeddings']\n",
    "image.shape\n",
    "\n",
    "def crop(image, output_size):\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    new_h, new_w = output_size\n",
    "\n",
    "    top = np.random.randint(0, h - new_h)\n",
    "    left = np.random.randint(0, w - new_w)\n",
    "\n",
    "    image = image[top:top + new_h, left:left + new_w]\n",
    "    \n",
    "    return image\n",
    "\n",
    "crop_image = crop(rescale(image[:102], (256, 256)), (224, 224))\n",
    "crop_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdabe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eaeddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d65e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "686e8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# # local functions\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "class DepressionDataset(Dataset):\n",
    "    '''create a training, develop, or test dataset\n",
    "       and load the participant features if it's called\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 mode,\n",
    "                 transform=None):\n",
    "        super(DepressionDataset, self).__init__()\n",
    "\n",
    "        # only train, develop, test dataset allow\n",
    "        assert mode in [\"train\", \"validation\", \"test\"], \\\n",
    "            \"Argument --mode could only be ['train', 'validation', 'test']\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train_data_path = os.path.join(self.root_dir, 'train_split_Depression_AVEC2017.csv')\n",
    "        self.valid_data_path = os.path.join(self.root_dir, 'dev_split_Depression_AVEC2017.csv')\n",
    "        self.test_data_path = os.path.join(self.root_dir, 'full_test_split.csv')\n",
    "        # load sent2vec model for converting text file to 2D array\n",
    "        self.sent2vec = SentenceTransformer('all-mpnet-base-v2')  # output dimension 768\n",
    "\n",
    "        # load training data # 107 sessions\n",
    "        if self.mode == \"train\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.train_data_path))\n",
    "            # store ground truth\n",
    "            ####################################################################################################\n",
    "            # self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.patientIDs = np.array([303, 321, 362, 363, 426])  # for debugging on my laptop\n",
    "            ####################################################################################################\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load development data # 35 sessions\n",
    "        if self.mode == \"validation\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.valid_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load test data # 47 sessions\n",
    "        if self.mode == \"test\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.test_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = None\n",
    "\n",
    "    def pre_check(self, data):\n",
    "        '''\n",
    "        Basic cleaning process to make sure no missing value\n",
    "        and that the sum of each PHQ subscore equals to PHQ score\n",
    "        Argument:\n",
    "            data: numpy array\n",
    "        Return:\n",
    "            data: numpy array with type \"int\"\n",
    "        '''\n",
    "        # make sure no NaN, Inf, -Inf\n",
    "        if data.isin([np.nan, np.inf, -np.inf]).any(1).sum():\n",
    "            print('Replacing NaN, Inf, or -Inf ...')\n",
    "            data = data.replace([np.inf, -np.inf, np.nan], 0)  # .astype('int')\n",
    "        else:\n",
    "            data = data  # .astype('int')\n",
    "\n",
    "        # compare the sum of each PHQ subscore to PHQ score\n",
    "        unequal = data.iloc[:, 4:].sum(axis=1) != data.iloc[:, 2]\n",
    "        if unequal.any() and self.mode != 'test':\n",
    "            lines = np.where(unequal)\n",
    "            raise ValueError((\"The sum of each PHQ subscore at line {} \"\n",
    "                              \"is unequal to the PHQ score\").format(lines[0]))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patientIDs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.patientIDs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Essentional function for creating dataset in PyTorch, which will automatically be\n",
    "        called in Dataloader and load all the extracted features of the patient in the Batch\n",
    "        based on the index of self.patientIDs\n",
    "        Argument:\n",
    "            idx: int, index of the patient ID in self.patientIDs\n",
    "        Return:\n",
    "            session: dict, contains all the extracted features and ground truth of a patient/session\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # get the patient session path\n",
    "        session_num = self.patientIDs[idx]\n",
    "        session_path = os.path.join(self.root_dir, '{}_P'.format(session_num))\n",
    "\n",
    "        # TODO: if other feature is needed, add more in the following part...\n",
    "\n",
    "        # get text path\n",
    "        text_path = os.path.join(session_path, '{}_TRANSCRIPT.csv'.format(session_num))\n",
    "\n",
    "        # text feature\n",
    "        self.text_feature = self.load_sent2vec(text_path, speaker='Participant')\n",
    "        sentence_embedding = self.text_feature['sentence_embeddings']\n",
    "\n",
    "        # summary\n",
    "        session = {'patientID': session_num,\n",
    "                   'session_path': session_path,\n",
    "                   'sentence_embeddings': sentence_embedding,\n",
    "                   'phq_score_gt': self.phq_score_gt[idx],\n",
    "                   'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "                   'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "                   'gender_gt': self.gender_gt[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            session = self.transform(session)\n",
    "\n",
    "        return session\n",
    "\n",
    "    def load_sent2vec(self, text_path, speaker='Participant'):\n",
    "        '''\n",
    "        load the text file and use sent2vec model from SentenceTransformer\n",
    "        for sentence embeddings, which generates 2D array\n",
    "        Arguments:\n",
    "            text_path: string, absolute path to transcipt file\n",
    "            speaker: certain string, which transcript of the speaker to load\n",
    "        Return:\n",
    "            text_feature: dict, contain converted embedding vectors, sentences, etc.\n",
    "        '''\n",
    "\n",
    "        # only 'Ellie', 'Participant', 'both' are allow\n",
    "        assert speaker in ['Ellie', 'Participant', 'both'], \\\n",
    "            \"Argument --speaker could only be ['Ellie', 'Participant', 'both']\"\n",
    "\n",
    "        text_file = pd.read_csv(text_path)\n",
    "        # tokenize the text file, filter out all \\t space and unnecessary columns such as time, participent\n",
    "        tokenized_words = self.tokenize_corpus(text_file.values.tolist()[i][0] for i in range(text_file.shape[0]))\n",
    "\n",
    "        sentences = []\n",
    "        sentences_idx = []\n",
    "\n",
    "        if speaker == 'Ellie':\n",
    "            for idx, sentence in enumerate(tokenized_words):\n",
    "                if sentence[2] == 'Ellie':\n",
    "                    sentences.append(sentence[3:])\n",
    "                    sentences_idx.append(idx)\n",
    "        elif speaker == 'Participant':\n",
    "            for idx, sentence in enumerate(tokenized_words):\n",
    "                if sentence[2] == 'Participant':\n",
    "                    sentences.append(sentence[3:])\n",
    "                    sentences_idx.append(idx)\n",
    "\n",
    "        else:  # speaker == 'both'\n",
    "            sentences = [tokenized_words[i][3:] for i in range(len(tokenized_words))]\n",
    "            sentences_idx = list(range(len(tokenized_words)))\n",
    "\n",
    "        # recombine 2D list of words into 1D list of sentence\n",
    "        final_sentences = [\" \".join(sentences[i]).lower() for i in range(len(sentences))]\n",
    "        # convert sentence to vector with SentenceTransformer pretrained model\n",
    "        sentence_embeddings = self.sent2vec.encode(final_sentences)\n",
    "\n",
    "        # summary\n",
    "        text_feature = {'speaker': speaker,\n",
    "                        'sentence_embeddings': sentence_embeddings,\n",
    "                        'sentences': final_sentences,\n",
    "                        'indices': sentences_idx}\n",
    "\n",
    "        return text_feature\n",
    "\n",
    "    def tokenize_corpus(self, corpus):\n",
    "        '''tokenzie a given list of string into list of words\n",
    "        Argument:\n",
    "            corpus: 1D list of string, each element is a sting of sentence\n",
    "        Return:\n",
    "            tokens: 2D list of string, each raw is a list of words splitted from sentence\n",
    "        '''\n",
    "        tokens = [x.split() for x in corpus]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class Padding(object):\n",
    "    ''' pad zero to each feature matrix so that they all have the same size '''\n",
    "\n",
    "    def __init__(self, text_output_size=(386, 768)):\n",
    "        super(Padding, self).__init__()\n",
    "        '''\n",
    "        Each output size could be 'int' or 'tuple'. \n",
    "        Integer would be the number of desired rows\n",
    "        and Tuple would be the desired 2D array size.\n",
    "\n",
    "        Here is recommended to keep the number of columns \n",
    "        as they are and only set the number of rows with int\n",
    "\n",
    "        To find the maximum length of rows, please use the \n",
    "        'find_max_length' function in utils to search through. \n",
    "\n",
    "        The value 386 are the maximum length in our case.\n",
    "        '''\n",
    "        assert isinstance(text_output_size, (int, tuple))\n",
    "        self.text_output_size = text_output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings']\n",
    "\n",
    "        # text padding\n",
    "        if isinstance(self.text_output_size, int):\n",
    "            shape = sentence_embeddings.shape\n",
    "            assert self.text_output_size >= shape[0], \\\n",
    "                \"audio output size should be bigger than {}\".format(shape[0])\n",
    "            padded_text = np.zeros((self.text_output_size, shape[1]))\n",
    "            padded_text[:shape[0], :shape[1]] = sentence_embeddings\n",
    "        else:\n",
    "            shape = sentence_embeddings.shape\n",
    "            assert self.text_output_size[0] >= shape[0] and self.text_output_size[1] >= shape[1], \\\n",
    "                \"text output size should be bigger than {}\".format(shape)\n",
    "            padded_text = np.zeros(self.text_output_size)\n",
    "            padded_text[:shape[0], :shape[1]] = sentence_embeddings\n",
    "\n",
    "        # summary\n",
    "        padded_session = {'patientID': session['patientID'],\n",
    "                          'session_path': session['session_path'],\n",
    "                          'sentence_embeddings': padded_text,\n",
    "                          'phq_score_gt': session['phq_score_gt'],\n",
    "                          'phq_binay_gt': session['phq_binay_gt'],\n",
    "                          'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                          'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return padded_session\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int),  Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=(256, 256)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings'].squeeze()  # 2D = (sentences number x 768)\n",
    "\n",
    "        h, w = sentence_embeddings.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        rescaled_sentence_embeddings = transform.resize(sentence_embeddings, (new_h, new_w))\n",
    "\n",
    "        # summary\n",
    "        rescaled_session = {'patientID': session['patientID'],\n",
    "                            'session_path': session['session_path'],\n",
    "                            'sentence_embeddings': rescaled_sentence_embeddings.reshape(1, new_h, new_w),\n",
    "                            'phq_score_gt': session['phq_score_gt'],\n",
    "                            'phq_binay_gt': session['phq_binay_gt'],\n",
    "                            'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                            'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return rescaled_session\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int), Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size=(224, 224)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        sentence_embeddings = session['sentence_embeddings'].squeeze()\n",
    "        \n",
    "        h, w = sentence_embeddings.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        cropped_sentence_embeddings = sentence_embeddings[top:top + new_h, left:left + new_w]\n",
    "\n",
    "        # summary\n",
    "        cropped_session = {'patientID': session['patientID'],\n",
    "                           'session_path': session['session_path'],\n",
    "                           'sentence_embeddings': cropped_sentence_embeddings.reshape(1, new_h, new_w),\n",
    "                           'phq_score_gt': session['phq_score_gt'],\n",
    "                           'phq_binay_gt': session['phq_binay_gt'],\n",
    "                           'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                           'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return cropped_session\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors or np.int to torch.tensor.\"\"\"\n",
    "\n",
    "    def __call__(self, session):\n",
    "\n",
    "        converted_session = {'patientID': session['patientID'],\n",
    "                             'session_path': session['session_path'],\n",
    "                             'sentence_embeddings': torch.from_numpy(session['sentence_embeddings']).type(torch.FloatTensor),\n",
    "                             'phq_score_gt': torch.tensor(session['phq_score_gt']).type(torch.FloatTensor),\n",
    "                             'phq_binay_gt': torch.tensor(session['phq_binay_gt']).type(torch.FloatTensor),\n",
    "                             'phq_subscores_gt': torch.from_numpy(session['phq_subscores_gt']).type(torch.FloatTensor),\n",
    "                             'gender_gt': torch.tensor(session['gender_gt']).type(torch.FloatTensor)}\n",
    "\n",
    "        return converted_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4817b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number:  0 , sentence embeddings:  torch.Size([2, 1, 224, 224])\n",
      "=================================\n",
      "Batch number:  1 , sentence embeddings:  torch.Size([2, 1, 224, 224])\n",
      "=================================\n",
      "Batch number:  2 , sentence embeddings:  torch.Size([1, 1, 224, 224])\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# sys.path.append('C:/Users/denni/Documents/KIT Studium/Bachelorarbeit')\n",
    "root_dir = 'C:/Users/denni/Documents/KIT Studium/Bachelorarbeit'\n",
    "\n",
    "# test 3: try to load the dataset with DataLoader\n",
    "transformed_dataset = DepressionDataset(os.path.join(root_dir, 'DAIC-WOZ Dataset'), 'train',\n",
    "                                        transform=transforms.Compose([Rescale((256, 256)),\n",
    "                                                                      RandomCrop((224, 224)),\n",
    "                                                                      ToTensor()]))\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(transformed_dataset,\n",
    "                        batch_size=2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=0)\n",
    "# iterate through batches\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print('Batch number: ', i_batch, ', sentence embeddings: ', sample_batched['sentence_embeddings'].size())\n",
    "    print('=================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49961f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b825f77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.99, 1.99, 2.99, 3.99]).type(torch.int)  # .to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d789fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.99, 1.99, 2.99, 3.99]).to(int).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d60dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8b84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf4fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb859188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455df23f",
   "metadata": {},
   "source": [
    "# Debug BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15935ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layer, dropout, bidirectional):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_size = input_dim         # 768 \n",
    "        self.num_class = output_dim             # 768\n",
    "        self.hidden_dim = hidden_dim            # 256\n",
    "        self.rnn_layers = num_layer             # 2 or 4\n",
    "        self.dropout = dropout                  # 0.2\n",
    "        self.bidirectional = bidirectional      # True\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # lstm layer\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dim,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "        \n",
    "        # FC layer\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dim, self.num_class),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "295ae0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = BiLSTM(input_dim=768,\n",
    "                hidden_dim=512, \n",
    "                output_dim=768, \n",
    "                num_layer=4, \n",
    "                dropout=0.2, \n",
    "                bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b58d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.randn(2, 10, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57b5b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bilstm(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51247907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1637dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eb174e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4c6891472149e68956c6cc136b14d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc28825d86984d28a6ac4c71649c9eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = transformers.BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d786df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "002994a8",
   "metadata": {},
   "source": [
    "# Debug Universal Sentence Transformer\n",
    "\n",
    "universal-sentence-encoder: https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "\n",
    "universal-sentence-encoder-large: https://tfhub.dev/google/universal-sentence-encoder-large/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93197a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb8b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c90458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentence_embedding(text_df, model):\n",
    "#     sentences = []\n",
    "#     for t in text_df.itertuples():\n",
    "#         if getattr(t, 'speaker') == 'Participant':\n",
    "#             if 'scrubbed_entry' in getattr(t,'value'):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sentences.append(getattr(t, 'value'))\n",
    "    \n",
    "#     return model.encode(sentences)\n",
    "\n",
    "def sentence_embedding(text_df):\n",
    "    sentences = []\n",
    "    for t in text_df.itertuples():\n",
    "        if getattr(t, 'speaker') == 'Participant':\n",
    "            if 'scrubbed_entry' in getattr(t,'value'):\n",
    "                continue\n",
    "            else:\n",
    "                sentences.append(getattr(t, 'value'))\n",
    "    \n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f32218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>speaker</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.96</td>\n",
       "      <td>11.93</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>hi i'm ellie thanks for coming in today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.67</td>\n",
       "      <td>16.11</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>i was created to talk to people in a safe and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.90</td>\n",
       "      <td>23.37</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>i'm not a therapist but i'm here to learn abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.93</td>\n",
       "      <td>28.64</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>and please feel free to tell me anything your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.16</td>\n",
       "      <td>30.47</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>are you okay with this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.91</td>\n",
       "      <td>31.37</td>\n",
       "      <td>Participant</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.84</td>\n",
       "      <td>34.90</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>so how are you doing today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35.39</td>\n",
       "      <td>35.78</td>\n",
       "      <td>Participant</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37.33</td>\n",
       "      <td>38.50</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>where are you from originally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39.44</td>\n",
       "      <td>40.33</td>\n",
       "      <td>Participant</td>\n",
       "      <td>from california</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  stop_time      speaker  \\\n",
       "0        8.96      11.93        Ellie   \n",
       "1       12.67      16.11        Ellie   \n",
       "2       16.90      23.37        Ellie   \n",
       "3       23.93      28.64        Ellie   \n",
       "4       29.16      30.47        Ellie   \n",
       "5       30.91      31.37  Participant   \n",
       "6       32.84      34.90        Ellie   \n",
       "7       35.39      35.78  Participant   \n",
       "8       37.33      38.50        Ellie   \n",
       "9       39.44      40.33  Participant   \n",
       "\n",
       "                                               value  \n",
       "0            hi i'm ellie thanks for coming in today  \n",
       "1  i was created to talk to people in a safe and ...  \n",
       "2  i'm not a therapist but i'm here to learn abou...  \n",
       "3  and please feel free to tell me anything your ...  \n",
       "4                            are you okay with this   \n",
       "5                                               yes   \n",
       "6                       so how are you doing today    \n",
       "7                                              good   \n",
       "8                     where are you from originally   \n",
       "9                                   from california   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_ID = 362\n",
    "text_path = os.path.join(os.getcwd(), 'DAIC-WOZ Dataset', f'{patient_ID}_P', f'{patient_ID}_TRANSCRIPT.csv')\n",
    "text_df = pd.read_csv(text_path, sep='\\t').fillna('')\n",
    "text_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587f7118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes ',\n",
       " 'good ',\n",
       " 'from california ',\n",
       " 'i like the eh arts and entertainments i like',\n",
       " 'the uh',\n",
       " 'you have a lot of freedom to work in a lot of different fields um',\n",
       " 'i like the fact that the weather is great um i like the fact that all my roots are here all my family is here  ',\n",
       " \"i don't like\",\n",
       " \"how hard it is to get places anymore it's very hard there's so much traffic it takes a long time to get where you're going um it's expensive it's a lot more expensive than it used to be  \",\n",
       " 'not anymore ',\n",
       " 'i studied journalism english music and special effects makeup ',\n",
       " 'i am ',\n",
       " 'now i am managing a doggy day care facility ',\n",
       " 'my dream job is to ',\n",
       " 'own my own company again um',\n",
       " 'probably working with animals ',\n",
       " 'no',\n",
       " \"i'm i'm an artist i draw i paint i do tattoos\",\n",
       " 'i work a lot with animals',\n",
       " 'i like outdoor activities biking hiking  ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentence_embedding(text_df)\n",
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1f4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b697de4",
   "metadata": {},
   "source": [
    "### Test universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4598fe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 512) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 512), dtype=float32, numpy=\n",
       "array([[ 0.00931179, -0.03993654,  0.04903579, ..., -0.05920121,\n",
       "        -0.01194051, -0.02619826],\n",
       "       [-0.0444302 , -0.07187285,  0.01583888, ..., -0.04185356,\n",
       "         0.01108887, -0.0329297 ],\n",
       "       [-0.02419742,  0.01430838,  0.00765016, ...,  0.02361106,\n",
       "         0.01626514, -0.00993789],\n",
       "       [ 0.04398917, -0.05413452,  0.08918855, ..., -0.02764365,\n",
       "         0.06124182, -0.06047855],\n",
       "       [-0.0489117 , -0.03056137, -0.01177089, ..., -0.07055238,\n",
       "         0.06200078,  0.02261064]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embeddings = embed(sentences)\n",
    "print(embeddings.shape, type(embeddings))\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c575d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 512) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05881099,  0.01910839,  0.01972206, ..., -0.01062876,\n",
       "         0.0303918 , -0.0292912 ],\n",
       "       [-0.02128776,  0.01480258,  0.02107059, ...,  0.01610595,\n",
       "        -0.06105837, -0.03500807],\n",
       "       [ 0.05227029,  0.05979312, -0.02633224, ..., -0.00835546,\n",
       "         0.0134875 , -0.02929783],\n",
       "       [ 0.06404437,  0.04930337,  0.03336997, ..., -0.01074252,\n",
       "         0.00155322,  0.04546057],\n",
       "       [ 0.05585645,  0.00434221,  0.02867533, ...,  0.00405687,\n",
       "        -0.10774966,  0.00071531]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_large = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "embeddings_large = embed_large(sentences)\n",
    "\n",
    "# convert from EagerTensor to numpy\n",
    "embeddings_large = embeddings_large.numpy()\n",
    "\n",
    "print(embeddings_large.shape, type(embeddings_large))\n",
    "embeddings_large[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0083d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
