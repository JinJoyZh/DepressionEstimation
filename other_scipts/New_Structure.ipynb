{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdaa6b8",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235b8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import transform\n",
    "# import librosa\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "\n",
    "# # local functions\n",
    "# from dataset.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb42cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionDataset(Dataset):\n",
    "    '''create a training, develop, or test dataset\n",
    "       and load the participant features if it's called \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 mode,\n",
    "                 transform=None):\n",
    "        super(DepressionDataset, self).__init__()\n",
    "        \n",
    "        # only train, develop, test dataset allow\n",
    "        assert mode in [\"train\", \"validation\", \"test\"], \\\n",
    "            \"Argument --mode could only be ['train', 'validation', 'test']\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train_data_path = os.path.join(self.root_dir, 'train_split_Depression_AVEC2017.csv')\n",
    "        self.valid_data_path = os.path.join(self.root_dir, 'dev_split_Depression_AVEC2017.csv')\n",
    "        self.test_data_path = os.path.join(self.root_dir, 'full_test_split.csv')\n",
    "        # load sent2vec model for converting text file to 2D array\n",
    "#         self.sent2vec = SentenceTransformer('all-mpnet-base-v2')  # output dimension 768\n",
    "\n",
    "        # load training data # 107 sessions\n",
    "        if self.mode == \"train\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.train_data_path))\n",
    "            # store ground truth\n",
    "            ####################################################################################################\n",
    "            # self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.patientIDs = np.array([303, 321, 362, 363, 426])  # for debugging on my laptop\n",
    "            ####################################################################################################\n",
    "            self.phq_binay_gt = np.array([0,1,1,0,1]) # self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load development data # 35 sessions\n",
    "        if self.mode == \"validation\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.valid_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load test data # 47 sessions\n",
    "        if self.mode == \"test\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.test_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            # subscores in test data are not provided, thus we initialize it with 0 to avoid error for DataLoader\n",
    "            self.phq_subscores_gt = np.zeros((self.patientIDs.shape[0],  8))\n",
    "        \n",
    "        \n",
    "        # get sampler\n",
    "        target = self.phq_binay_gt  # np.array([0,1,1,0,1])  # self.phq_binay_gt\n",
    "        class_sample_count = np.unique(target, return_counts=True)[1]\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = weight[target]\n",
    "        samples_weight = torch.from_numpy(samples_weight).double()\n",
    "        self.sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "    \n",
    "    def pre_check(self, data):\n",
    "        '''\n",
    "        Basic cleaning process to make sure no missing value\n",
    "        and that the sum of each PHQ subscore equals to PHQ score \n",
    "        Argument:\n",
    "            data: numpy array\n",
    "        Return:\n",
    "            data: numpy array with type \"int\"\n",
    "        '''\n",
    "        # make sure no NaN, Inf, -Inf\n",
    "        if data.isin([np.nan, np.inf, -np.inf]).any(1).sum():\n",
    "            print('Replacing NaN, Inf, or -Inf ...')\n",
    "            data = data.replace([np.inf, -np.inf, np.nan], 0).astype('int')\n",
    "        else: \n",
    "            data = data.astype('int')\n",
    "            \n",
    "        # compare the sum of each PHQ subscore to PHQ score\n",
    "        unequal = data.iloc[:, 4:].sum(axis=1) != data.iloc[:,2]\n",
    "        if unequal.any() and self.mode != 'test':\n",
    "            lines = np.where(unequal)\n",
    "            raise ValueError((\"The sum of each PHQ subscore at line {} \"\n",
    "                              \"is unequal to the PHQ score\").format(lines[0]))\n",
    "        \n",
    "        # check whether the PHQ binary is correctly converted based on PHQ score \n",
    "        phq_binary = data.iloc[:, 1].to_numpy()\n",
    "        phq_score = data.iloc[:, 2].to_numpy()\n",
    "        phq_converted_binary = np.where(phq_score > 9, 1, 0)\n",
    "        if (phq_converted_binary != phq_binary).any():\n",
    "            where = np.where(phq_converted_binary != phq_binary)\n",
    "            data.iloc[where, 1] = phq_converted_binary[where]\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patientIDs)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.patientIDs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Essentional function for creating dataset in PyTorch, which will automatically be\n",
    "        called in Dataloader and load all the extracted features of the patient in the Batch\n",
    "        based on the index of self.patientIDs\n",
    "        Argument:\n",
    "            idx: int, index of the patient ID in self.patientIDs\n",
    "        Return:\n",
    "            session: dict, contains all the extracted features and ground truth of a patient/session \n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # get the patient session path\n",
    "        session_num = self.patientIDs[idx]\n",
    "        session_path = os.path.join(self.root_dir, '{}_P'.format(session_num))\n",
    "        \n",
    "        # TODO: if other feature is needed, add more in the following part...\n",
    "        \n",
    "        # get key points and gaze direction path\n",
    "        facial_landmarks_path = os.path.join(session_path, '{}_CLNF_features3D.txt'.format(session_num))\n",
    "        gaze_direction_path = os.path.join(session_path, '{}_CLNF_gaze.txt'.format(session_num))\n",
    "        \n",
    "        # facial feature\n",
    "        facial_landmarks = self.load_facial_landmarks(facial_landmarks_path)\n",
    "        # gaze direction feature\n",
    "        gaze_direction = pd.read_csv(gaze_direction_path).iloc[:, 4:].to_numpy()\n",
    "        \n",
    "        # summary\n",
    "        session = {'patientID': session_num,\n",
    "                   'session_path': session_path,\n",
    "                   'facial_landmarks': facial_landmarks,\n",
    "                   'gaze_direction': gaze_direction,\n",
    "                   'phq_score_gt': self.phq_score_gt[idx],\n",
    "                   'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "                   'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "                   'gender_gt': self.gender_gt[idx]}\n",
    "        \n",
    "#         # get all features path of the session\n",
    "#         facial_landmarks_path = os.path.join(session_path, '{}_CLNF_features3D.txt'.format(session_num))\n",
    "#         gaze_direction_path = os.path.join(session_path, '{}_CLNF_gaze.txt'.format(session_num))\n",
    "#         audio_path = os.path.join(session_path, '{}_AUDIO.wav'.format(session_num))\n",
    "#         text_path = os.path.join(session_path, '{}_TRANSCRIPT.csv'.format(session_num))\n",
    "        \n",
    "#         # facial feature\n",
    "#         facial_landmarks = self.load_facial_landmarks(facial_landmarks_path)\n",
    "#         # gaze direction feature\n",
    "#         gaze_direction = pd.read_csv(gaze_direction_path).iloc[:, 4:].to_numpy()\n",
    "#         # audion feature, but constrain the rows based to match the shape of landmarks/gaze_sample\n",
    "#         audio, self.audio_parameters = self.load_audio(audio_path)\n",
    "#         audio = audio[:facial_landmarks.shape[0]]\n",
    "#         # text feature\n",
    "#         self.text_feature = self.load_sent2vec(text_path, speaker='Participant')\n",
    "#         sentence_embedding = self.text_feature['sentence_embeddings']\n",
    "\n",
    "#         # summary\n",
    "#         session = {'patientID': session_num,\n",
    "#                    'session_path': session_path,\n",
    "#                    'facial_landmarks': facial_landmarks,\n",
    "#                    'gaze_direction': gaze_direction, \n",
    "#                    'audio': audio,\n",
    "#                    'sentence_embeddings': sentence_embedding,\n",
    "#                    'phq_score_gt': self.phq_score_gt[idx],\n",
    "#                    'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "#                    'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "#                    'gender_gt': self.gender_gt[idx]}\n",
    "        \n",
    "        if self.transform:\n",
    "            session = self.transform(session)\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    \n",
    "    def load_facial_landmarks(self, facial_landmarks_path, preprocess=True):\n",
    "        ''' \n",
    "        load the facial landmarks and separately recale \n",
    "        each x, y, z coordiante of each frame\n",
    "        Arguments:\n",
    "            facial_landmarks_path: string, absolute path to 3D facial landmarks file\n",
    "            preprocess: boolean, whether normalize the data\n",
    "        Return:\n",
    "            landmarks: 2D numpy.ndarray, coordinate (x,y,z) of 68 3D facial points\n",
    "        '''\n",
    "        # load the landmarks file\n",
    "        landmarks = pd.read_csv(facial_landmarks_path).iloc[:, 4:].to_numpy()\n",
    "        \n",
    "        if preprocess:\n",
    "            # recale x, y, z\n",
    "            landmarks_x = minmax_scaler(landmarks[:, 0:68])\n",
    "            landmarks_y = minmax_scaler(landmarks[:, 68:136])\n",
    "            landmarks_z = minmax_scaler(landmarks[:, 136:204])\n",
    "            # concatenate together\n",
    "            landmarks = np.concatenate([landmarks_x, landmarks_y, landmarks_z], axis=1)\n",
    "        \n",
    "        return landmarks\n",
    "    \n",
    "\n",
    "class Padding(object):\n",
    "    ''' pad zero to each feature matrix so that they all have the same size '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 landmarks_output_size=(58989, 204), \n",
    "                 gaze_output_size=(58989, 12)):\n",
    "        super(Padding, self).__init__()\n",
    "        '''\n",
    "        Each output size could be 'int' or 'tuple'. \n",
    "        Integer would be the number of desired rows\n",
    "        and Tuple would be the desired 2D array size.\n",
    "\n",
    "        Here is recommended to keep the number of columns \n",
    "        as they are and only set the number of rows with int\n",
    "\n",
    "        To find the maximum length of rows, please use the \n",
    "        'find_max_length' function in utils to search through. \n",
    "\n",
    "        The value 386 are the maximum length in our case.\n",
    "        '''\n",
    "        assert isinstance(landmarks_output_size, (int, tuple))\n",
    "        assert isinstance(gaze_output_size, (int, tuple))\n",
    "        \n",
    "        self.landmarks_output_size = landmarks_output_size\n",
    "        self.gaze_output_size = gaze_output_size\n",
    "\n",
    "        \n",
    "    def __call__(self, session):\n",
    "        facial_landmarks = session['facial_landmarks']\n",
    "        gaze_direction = session['gaze_direction']\n",
    "        \n",
    "        # facial landmarks padding along heigh dimension (dim-0)\n",
    "        if isinstance(self.landmarks_output_size, int):\n",
    "            h, w = facial_landmarks.shape\n",
    "            new_h = self.landmarks_output_size if h > self.landmarks_output_size else h\n",
    "            padded_landmarks = np.zeros((self.landmarks_output_size, w))\n",
    "            padded_landmarks[:new_h, :w] = facial_landmarks[:new_h, :w]\n",
    "        # facial landmarks padding along both heigh and width dimension\n",
    "        else:\n",
    "            h, w = facial_landmarks.shape\n",
    "            new_h = self.landmarks_output_size[0] if h > self.landmarks_output_size[0] else h\n",
    "            new_w = self.landmarks_output_size[1] if w > self.landmarks_output_size[1] else w\n",
    "            padded_landmarks = np.zeros(self.landmarks_output_size)\n",
    "            padded_landmarks[:new_h, :new_w] = facial_landmarks[:new_h, :new_w]\n",
    "            \n",
    "        # gaze direction padding along heigh dimension (dim-0)\n",
    "        if isinstance(self.gaze_output_size, int):\n",
    "            h, w = gaze_direction.shape\n",
    "            new_h = self.gaze_output_size if h > self.gaze_output_size else h\n",
    "            padded_gaze = np.zeros((self.gaze_output_size, w))\n",
    "            padded_gaze[:new_h, :w] = gaze_direction[:new_h, :w]\n",
    "        # gaze direction padding along both heigh and width dimension\n",
    "        else:\n",
    "            h, w = gaze_direction.shape\n",
    "            new_h = self.gaze_output_size[0] if h > self.gaze_output_size[0] else h\n",
    "            new_w = self.gaze_output_size[1] if w > self.gaze_output_size[1] else w\n",
    "            padded_gaze = np.zeros(self.gaze_output_size)\n",
    "            padded_gaze[:new_h, :new_w] = gaze_direction[:new_h, :new_w]\n",
    "\n",
    "        # summary\n",
    "        padded_session = {'patientID': session['patientID'],\n",
    "                          'session_path': session['session_path'],\n",
    "                          'facial_landmarks': padded_landmarks,\n",
    "                          'gaze_direction': padded_gaze, \n",
    "                          'phq_score_gt': session['phq_score_gt'],\n",
    "                          'phq_binay_gt': session['phq_binay_gt'],\n",
    "                          'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                          'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return padded_session\n",
    "\n",
    "    \n",
    "# class Rescale(object):\n",
    "#     \"\"\"Rescale the image in a sample to a given size.\n",
    "#     Arguments:\n",
    "#         output_size:(tuple or int),  Desired output size. If tuple, output is\n",
    "#             matched to output_size. If int, smaller of image edges is matched\n",
    "#             to output_size keeping aspect ratio the same.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, output_size=(256, 256)):\n",
    "#         assert isinstance(output_size, (int, tuple))\n",
    "#         self.output_size = output_size\n",
    "\n",
    "#     def __call__(self, session):\n",
    "#         audio = session['audio']\n",
    "\n",
    "#         h, w = audio.shape[:2]\n",
    "\n",
    "#         if isinstance(self.output_size, int):\n",
    "#             if h > w:\n",
    "#                 new_h, new_w = self.output_size * h / w, self.output_size\n",
    "#             else:\n",
    "#                 new_h, new_w = self.output_size, self.output_size * w / h\n",
    "#         else:\n",
    "#             new_h, new_w = self.output_size\n",
    "\n",
    "#         new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "#         rescaled_audio = transform.resize(audio, (new_h, new_w))\n",
    "\n",
    "#         # summary\n",
    "#         rescaled_session = {'patientID': session['patientID'],\n",
    "#                             'session_path': session['session_path'],\n",
    "#                             'audio': rescaled_audio,\n",
    "#                             'phq_score_gt': session['phq_score_gt'],\n",
    "#                             'phq_binay_gt': session['phq_binay_gt'],\n",
    "#                             'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "#                             'gender_gt': session['gender_gt']}\n",
    "\n",
    "#         return rescaled_session\n",
    "\n",
    "\n",
    "# class RandomCrop(object):\n",
    "#     \"\"\"Crop randomly the image in a sample.\n",
    "#     Arguments:\n",
    "#         output_size:(tuple or int), Desired output size. If int, square crop\n",
    "#             is made.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, output_size=(224, 224)):\n",
    "#         assert isinstance(output_size, (int, tuple))\n",
    "\n",
    "#         if isinstance(output_size, int):\n",
    "#             self.output_size = (output_size, output_size)\n",
    "#         else:\n",
    "#             assert len(output_size) == 2\n",
    "#             self.output_size = output_size\n",
    "\n",
    "#     def __call__(self, session):\n",
    "#         audio = session['audio']\n",
    "\n",
    "#         h, w = audio.shape[:2]\n",
    "#         new_h, new_w = self.output_size\n",
    "\n",
    "#         top = np.random.randint(0, h - new_h)\n",
    "#         left = np.random.randint(0, w - new_w)\n",
    "\n",
    "#         cropped_audio = audio[top:top + new_h, left:left + new_w]\n",
    "\n",
    "#         # summary\n",
    "#         cropped_session = {'patientID': session['patientID'],\n",
    "#                            'session_path': session['session_path'],\n",
    "#                            'audio': cropped_audio,\n",
    "#                            'phq_score_gt': session['phq_score_gt'],\n",
    "#                            'phq_binay_gt': session['phq_binay_gt'],\n",
    "#                            'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "#                            'gender_gt': session['gender_gt']}\n",
    "\n",
    "#         return cropped_session\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors or np.int to torch.tensor.\"\"\"\n",
    "\n",
    "    def __call__(self, session):\n",
    "        \n",
    "        facial_landmarks = session['facial_landmarks']\n",
    "        gaze_direction = session['gaze_direction']\n",
    "        \n",
    "        converted_session = {'patientID': session['patientID'],\n",
    "                             'session_path': session['session_path'],\n",
    "                             'facial_landmarks': torch.from_numpy(session['facial_landmarks']).type(torch.FloatTensor),\n",
    "                             'gaze_direction': torch.from_numpy(session['gaze_direction']).type(torch.FloatTensor),\n",
    "                             'phq_score_gt': torch.tensor(session['phq_score_gt']).type(torch.FloatTensor),\n",
    "                             'phq_binay_gt': torch.tensor(session['phq_binay_gt']).type(torch.FloatTensor),\n",
    "                             'phq_subscores_gt': torch.from_numpy(session['phq_subscores_gt']).type(torch.FloatTensor),\n",
    "                             'gender_gt': torch.tensor(session['gender_gt']).type(torch.FloatTensor)}\n",
    "\n",
    "        return converted_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea9123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2184ee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>487</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID  PHQ8_Binary  PHQ8_Score  Gender  PHQ8_NoInterest  \\\n",
       "0               303            0           0       0                0   \n",
       "1               304            0           6       0                0   \n",
       "2               305            0           7       1                0   \n",
       "3               310            0           4       1                1   \n",
       "4               312            0           2       1                0   \n",
       "..              ...          ...         ...     ...              ...   \n",
       "102             485            0           2       1                0   \n",
       "103             486            0           4       0                1   \n",
       "104             487            0           0       0                0   \n",
       "105             488            0           0       0                0   \n",
       "106             491            0           8       0                1   \n",
       "\n",
       "     PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  PHQ8_Appetite  PHQ8_Failure  \\\n",
       "0                 0           0           0              0             0   \n",
       "1                 1           1           2              2             0   \n",
       "2                 1           1           2              2             1   \n",
       "3                 1           0           0              0             1   \n",
       "4                 0           1           1              0             0   \n",
       "..              ...         ...         ...            ...           ...   \n",
       "102               1           0           0              0             0   \n",
       "103               1           0           1              0             1   \n",
       "104               0           0           0              0             0   \n",
       "105               0           0           0              0             0   \n",
       "106               1           1           1              1             1   \n",
       "\n",
       "     PHQ8_Concentrating  PHQ8_Moving  \n",
       "0                     0            0  \n",
       "1                     0            0  \n",
       "2                     0            0  \n",
       "3                     1            0  \n",
       "4                     0            0  \n",
       "..                  ...          ...  \n",
       "102                   0            1  \n",
       "103                   0            0  \n",
       "104                   0            0  \n",
       "105                   0            0  \n",
       "106                   1            1  \n",
       "\n",
       "[107 rows x 12 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_check(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e41c72db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Participant_ID        409\n",
       "PHQ8_Binary             0\n",
       "PHQ8_Score             10\n",
       "Gender                  1\n",
       "PHQ8_NoInterest         1\n",
       "PHQ8_Depressed          1\n",
       "PHQ8_Sleep              2\n",
       "PHQ8_Tired              3\n",
       "PHQ8_Appetite           0\n",
       "PHQ8_Failure            1\n",
       "PHQ8_Concentrating      2\n",
       "PHQ8_Moving             0\n",
       "Name: 64, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5cb9d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_NoInterest</th>\n",
       "      <th>PHQ8_Depressed</th>\n",
       "      <th>PHQ8_Sleep</th>\n",
       "      <th>PHQ8_Tired</th>\n",
       "      <th>PHQ8_Appetite</th>\n",
       "      <th>PHQ8_Failure</th>\n",
       "      <th>PHQ8_Concentrating</th>\n",
       "      <th>PHQ8_Moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>487</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant_ID  PHQ8_Binary  PHQ8_Score  Gender  PHQ8_NoInterest  \\\n",
       "0               303            0           0       0                0   \n",
       "1               304            0           6       0                0   \n",
       "2               305            0           7       1                0   \n",
       "3               310            0           4       1                1   \n",
       "4               312            0           2       1                0   \n",
       "..              ...          ...         ...     ...              ...   \n",
       "102             485            0           2       1                0   \n",
       "103             486            0           4       0                1   \n",
       "104             487            0           0       0                0   \n",
       "105             488            0           0       0                0   \n",
       "106             491            0           8       0                1   \n",
       "\n",
       "     PHQ8_Depressed  PHQ8_Sleep  PHQ8_Tired  PHQ8_Appetite  PHQ8_Failure  \\\n",
       "0                 0           0           0              0             0   \n",
       "1                 1           1           2              2             0   \n",
       "2                 1           1           2              2             1   \n",
       "3                 1           0           0              0             1   \n",
       "4                 0           1           1              0             0   \n",
       "..              ...         ...         ...            ...           ...   \n",
       "102               1           0           0              0             0   \n",
       "103               1           0           1              0             1   \n",
       "104               0           0           0              0             0   \n",
       "105               0           0           0              0             0   \n",
       "106               1           1           1              1             1   \n",
       "\n",
       "     PHQ8_Concentrating  PHQ8_Moving  \n",
       "0                     0            0  \n",
       "1                     0            0  \n",
       "2                     0            0  \n",
       "3                     1            0  \n",
       "4                     0            0  \n",
       "..                  ...          ...  \n",
       "102                   0            1  \n",
       "103                   0            0  \n",
       "104                   0            0  \n",
       "105                   0            0  \n",
       "106                   1            1  \n",
       "\n",
       "[107 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9dcf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join('C:/Users/denni/Documents/KIT Studium/Bachelorarbeit', 'DAIC-WOZ Dataset')\n",
    "\n",
    "train_data_path = os.path.join(root_dir, 'train_split_Depression_AVEC2017.csv')\n",
    "valid_data_path = os.path.join(root_dir, 'dev_split_Depression_AVEC2017.csv')\n",
    "test_data_path = os.path.join(root_dir, 'full_test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91c6ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "valid_data = pd.read_csv(valid_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "97cfdf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((107,), (35,), (47,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_patient_IDs = train_data['Participant_ID'].to_numpy().astype(np.int16)\n",
    "valid_patient_IDs = valid_data['Participant_ID'].to_numpy().astype(np.int16)\n",
    "test_patient_IDs = test_data['Participant_ID'].to_numpy().astype(np.int16)\n",
    "\n",
    "train_patient_IDs.shape, valid_patient_IDs.shape, test_patient_IDs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "499da8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "        313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "        339, 340, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
       "        353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
       "        366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "        392, 393, 395, 396, 397, 399, 400, 401, 402, 403, 404, 405, 406,\n",
       "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "        433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
       "        446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
       "        459, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
       "        473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485,\n",
       "        486, 487, 488, 489, 490, 491, 492], dtype=int16),\n",
       " (189,))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_patient_IDs = np.sort(np.hstack([train_patient_IDs, valid_patient_IDs, test_patient_IDs])).astype(np.int16)\n",
    "\n",
    "full_patient_IDs, full_patient_IDs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd7334a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int16"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full_patient_IDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca12eb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([106], dtype=int64),)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(full_patient_IDs==409)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0cd25e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phq_converted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1dc74c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
    "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
    "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
    "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
    "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
    "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "len(a)\n",
    "a[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "685ac74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio file\n",
    "import h5py\n",
    "\n",
    "\n",
    "audio_root = 'D:/DAIC-WOZ_dataset/Audio/logmel_snv_exp'\n",
    "audio_path = os.path.join(audio_root, 'complete_database.h5')\n",
    "\n",
    "\n",
    "with h5py.File(audio_path, 'r') as h5:\n",
    "    features = h5['features'][:, 0]\n",
    "    labels = h5['class'][:]\n",
    "    scores = h5['score'][:]\n",
    "    patientIDs = h5['folder'][:]\n",
    "    index = h5['index'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94baf3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
       "       353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
       "       366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "       379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "       392, 393, 395, 396, 397, 399, 400, 401, 402, 403, 404, 405, 406,\n",
       "       407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
       "       446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
       "       459, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
       "       473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485,\n",
       "       486, 487, 488, 489, 490, 491, 492], dtype=int16)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patientIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "279d40ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   5,  10,  12,  13,  15,  16,  17,  18,  19,  20,  21,\n",
       "        22,  24,  25,  26,  27,  28,  30,  33,  36,  38,  39,  40,  41,\n",
       "        42,  43,  44,  46,  47,  49,  50,  51,  52,  54,  55,  56,  57,\n",
       "        59,  61,  62,  63,  65,  67,  68,  69,  70,  71,  73,  74,  75,\n",
       "        78,  79,  82,  84,  85,  90,  91,  92,  95,  97,  98,  99, 106,\n",
       "       109, 111, 112, 113, 116, 120, 122, 123, 124, 125, 126, 127, 130,\n",
       "       131, 134, 138, 140, 141, 142, 143, 144, 145, 146, 151, 152, 153,\n",
       "       154, 156, 159, 160, 164, 167, 169, 170, 171, 174, 175, 181, 182,\n",
       "       183, 184, 187], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.where(full_patient_IDs==i) for i in train_patient_IDs]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c331a37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_in_whole_dataset = np.where(full_patient_IDs==303)[0][0]\n",
    "idx_in_whole_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "00fc3270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32743   ,  1.3249494 ,  1.2658678 , ...,  1.2685912 ,\n",
       "         1.392497  ,  1.4017674 ],\n",
       "       [ 1.107519  ,  1.202885  ,  1.1357735 , ...,  0.80705774,\n",
       "         0.7334303 ,  0.7568188 ],\n",
       "       [ 1.2476406 ,  0.8866333 ,  0.44323593, ...,  0.65998703,\n",
       "         0.6704184 ,  0.39679855],\n",
       "       ...,\n",
       "       [-1.5545362 , -1.5219041 , -1.6075579 , ..., -1.7362987 ,\n",
       "        -1.6579329 , -1.7135811 ],\n",
       "       [-1.7514281 , -1.7431799 , -1.7791928 , ..., -1.8250825 ,\n",
       "        -1.8601888 , -1.9085772 ],\n",
       "       [-1.8333642 , -1.91712   , -1.9631115 , ..., -2.0160012 ,\n",
       "        -1.976898  , -1.998173  ]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[idx_in_whole_dataset].reshape(80, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81a591a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 1.32743  ,  1.3249494,  1.2658678, ..., -2.0160012, -1.976898 ,\n",
       "              -1.998173 ], dtype=float32)                                     ,\n",
       "       array([ 1.0601646,  1.2481416,  1.3038456, ..., -1.7629714, -1.7911508,\n",
       "              -1.7723138], dtype=float32)                                     ,\n",
       "       array([ 1.2560773,  1.0517627,  0.9097564, ..., -1.3292217, -1.3368998,\n",
       "              -1.4047761], dtype=float32)                                     ,\n",
       "       array([ 1.5073284,  1.4591094,  1.2482179, ..., -1.1067563, -1.0654975,\n",
       "              -1.1343845], dtype=float32)                                     ,\n",
       "       array([ 1.8171296,  1.8722442,  1.730458 , ..., -1.416058 , -1.4015712,\n",
       "              -1.4170307], dtype=float32)                                     ,\n",
       "       array([ 0.82127744,  0.882391  ,  0.9713714 , ..., -1.4978169 ,\n",
       "              -1.4576048 , -1.418537  ], dtype=float32)               ,\n",
       "       array([ 1.3755808,  1.2391801,  1.3063501, ..., -1.4128875, -1.3898149,\n",
       "              -1.3465071], dtype=float32)                                     ,\n",
       "       array([ 1.8045506,  2.71243  ,  3.021769 , ..., -1.462981 , -1.4419324,\n",
       "              -1.5091857], dtype=float32)                                     ,\n",
       "       array([ 1.3588662,  1.546522 ,  1.7204884, ..., -1.4560304, -1.4174324,\n",
       "              -1.3751696], dtype=float32)                                     ,\n",
       "       array([ 1.1626213,  1.1880243,  1.1570123, ..., -1.7780316, -1.850648 ,\n",
       "              -1.8596529], dtype=float32)                                     ,\n",
       "       array([ 0.96623766,  0.9071603 ,  1.2189177 , ..., -1.6964813 ,\n",
       "              -1.7942281 , -1.7512695 ], dtype=float32)               ,\n",
       "       array([ 2.0060644,  1.9964437,  1.5995953, ..., -1.612711 , -1.5480165,\n",
       "              -1.447812 ], dtype=float32)                                     ,\n",
       "       array([ 1.1969148 ,  1.0558246 ,  0.96908134, ..., -1.7335354 ,\n",
       "              -1.7485373 , -1.7845525 ], dtype=float32)               ,\n",
       "       array([ 1.980828 ,  1.7784325,  1.4291917, ..., -1.3517712, -1.4098384,\n",
       "              -1.283267 ], dtype=float32)                                     ,\n",
       "       array([ 2.1182773,  2.0698364,  1.9967487, ..., -1.7418289, -1.7972254,\n",
       "              -1.8515915], dtype=float32)                                     ,\n",
       "       array([ 1.4777542,  1.5724995,  1.2824674, ..., -1.987584 , -1.9071897,\n",
       "              -1.8190603], dtype=float32)                                     ,\n",
       "       array([ 1.4146849,  1.2954229,  1.236292 , ..., -1.762829 , -1.6942463,\n",
       "              -1.8012079], dtype=float32)                                     ,\n",
       "       array([ 1.1981529 ,  0.95218045,  0.89195824, ..., -1.6846011 ,\n",
       "              -1.6463504 , -1.675182  ], dtype=float32)               ,\n",
       "       array([ 0.86655265,  0.6518653 ,  0.5091014 , ..., -0.8167952 ,\n",
       "              -0.838554  , -0.9562743 ], dtype=float32)               ,\n",
       "       array([ 1.2786067,  1.140361 ,  1.2292618, ..., -1.494038 , -1.5795772,\n",
       "              -1.5758071], dtype=float32)                                     ,\n",
       "       array([ 1.4543155 ,  1.2598836 ,  0.93754405, ..., -0.9829665 ,\n",
       "              -1.3262905 , -1.6674865 ], dtype=float32)               ,\n",
       "       array([ 1.9103414,  1.597689 ,  0.9514927, ..., -1.3959558, -1.4887382,\n",
       "              -1.5327152], dtype=float32)                                     ,\n",
       "       array([-0.31873313, -0.33399516,  0.04296364, ..., -2.0129225 ,\n",
       "              -2.0250685 , -1.9873894 ], dtype=float32)               ,\n",
       "       array([ 0.5876175 ,  0.98798996,  1.592002  , ..., -1.762162  ,\n",
       "              -1.7146941 , -1.6064804 ], dtype=float32)               ,\n",
       "       array([ 1.2348673 ,  0.95001537,  0.88740355, ..., -1.6284301 ,\n",
       "              -1.4912374 , -1.7031379 ], dtype=float32)               ,\n",
       "       array([ 1.0345875,  1.0578357,  1.1394216, ..., -1.9404058, -1.8753629,\n",
       "              -1.8375919], dtype=float32)                                     ,\n",
       "       array([ 1.2880367,  1.071746 ,  1.1003019, ..., -1.453367 , -1.5675248,\n",
       "              -1.7324839], dtype=float32)                                     ,\n",
       "       array([ 1.5354749 ,  1.2662089 ,  1.0272481 , ..., -0.55746734,\n",
       "              -0.97569627, -1.4113294 ], dtype=float32)               ,\n",
       "       array([ 0.16863868,  0.22293843,  0.2575432 , ..., -2.2482378 ,\n",
       "              -2.190825  , -2.176145  ], dtype=float32)               ,\n",
       "       array([ 1.0347002,  1.1925504,  1.070033 , ..., -1.7258084, -1.7451017,\n",
       "              -1.760529 ], dtype=float32)                                     ,\n",
       "       array([ 1.3295571,  1.2843642,  0.9664658, ..., -1.0949372, -1.5595324,\n",
       "              -1.7567503], dtype=float32)                                     ,\n",
       "       array([ 1.9181409,  1.5834419,  1.2485259, ..., -1.8182077, -1.8412396,\n",
       "              -1.8378761], dtype=float32)                                     ,\n",
       "       array([ 0.88187766,  0.95307815,  0.87971   , ..., -1.8760294 ,\n",
       "              -1.9268628 , -1.9415675 ], dtype=float32)               ,\n",
       "       array([ 1.1345389,  1.185851 ,  1.2462516, ..., -1.4883666, -1.8883698,\n",
       "              -2.1403537], dtype=float32)                                     ,\n",
       "       array([ 1.2649848 ,  0.98452705,  0.62452066, ..., -1.8259661 ,\n",
       "              -1.7892455 , -1.8235582 ], dtype=float32)               ,\n",
       "       array([ 1.5645494,  1.2233866,  0.7519134, ..., -1.799427 , -1.6096594,\n",
       "              -1.5040156], dtype=float32)                                     ,\n",
       "       array([ 1.2784622,  1.4915617,  1.5960075, ..., -1.0505574, -1.4595311,\n",
       "              -1.5755725], dtype=float32)                                     ,\n",
       "       array([ 1.4338732 ,  1.1957209 ,  1.0833933 , ..., -0.14412934,\n",
       "              -0.2079901 , -0.22365537], dtype=float32)               ,\n",
       "       array([ 1.9761493,  1.7395555,  1.4262631, ..., -0.6607482, -1.0336367,\n",
       "              -1.4821695], dtype=float32)                                     ,\n",
       "       array([ 1.7535399,  1.4928039,  1.357087 , ..., -2.0256534, -2.188473 ,\n",
       "              -2.1510525], dtype=float32)                                     ,\n",
       "       array([ 1.6325109,  1.5938945,  1.5601038, ..., -1.7693726, -1.7436677,\n",
       "              -1.775403 ], dtype=float32)                                     ,\n",
       "       array([ 0.7567117 ,  0.7821727 ,  0.61595786, ..., -0.8104661 ,\n",
       "              -0.88909966, -1.0162683 ], dtype=float32)               ,\n",
       "       array([ 0.64239234,  0.70661426,  0.7799661 , ..., -2.0269003 ,\n",
       "              -2.0017467 , -1.9607075 ], dtype=float32)               ,\n",
       "       array([ 0.9016543 ,  0.79945475,  0.6031735 , ..., -1.5748448 ,\n",
       "              -1.482129  , -1.5020511 ], dtype=float32)               ,\n",
       "       array([ 1.1611373,  1.0757469,  1.0372702, ..., -1.7562889, -1.7451913,\n",
       "              -1.6764694], dtype=float32)                                     ,\n",
       "       array([ 0.190479 ,  0.5439556,  0.6152281, ..., -1.2948825, -1.0824172,\n",
       "              -1.241416 ], dtype=float32)                                     ,\n",
       "       array([ 0.6795673 ,  0.6657593 ,  0.69851536, ..., -1.0862904 ,\n",
       "              -1.2425642 , -1.2826685 ], dtype=float32)               ,\n",
       "       array([ 1.3711817,  1.3005347,  1.2461854, ..., -1.5542697, -1.679381 ,\n",
       "              -1.6628203], dtype=float32)                                     ,\n",
       "       array([ 0.8482847 ,  0.98889065,  0.96717787, ..., -1.4322896 ,\n",
       "              -1.4313958 , -1.5717456 ], dtype=float32)               ,\n",
       "       array([ 0.10914219,  0.1721276 ,  0.15932503, ..., -1.5342432 ,\n",
       "              -1.9067473 , -1.8015535 ], dtype=float32)               ,\n",
       "       array([ 1.607293 ,  1.4533823,  1.103043 , ..., -1.6263322, -1.7158252,\n",
       "              -1.75074  ], dtype=float32)                                     ,\n",
       "       array([ 1.3697573,  1.3965251,  1.446547 , ..., -0.95557  , -1.1341046,\n",
       "              -1.6904052], dtype=float32)                                     ,\n",
       "       array([ 0.33667842,  0.49765784,  0.53687173, ..., -0.58893305,\n",
       "              -1.0668465 , -2.005642  ], dtype=float32)               ,\n",
       "       array([ 0.30586413,  0.28409395,  0.34075823, ..., -2.0147772 ,\n",
       "              -1.9840991 , -1.8895618 ], dtype=float32)               ,\n",
       "       array([ 1.5362777 ,  1.4177523 ,  1.3151113 , ..., -0.55461866,\n",
       "              -1.0865526 , -0.9745131 ], dtype=float32)               ,\n",
       "       array([ 2.1258776,  2.1160152,  2.0798497, ..., -1.6251291, -1.6236737,\n",
       "              -1.5210598], dtype=float32)                                     ,\n",
       "       array([ 1.4292767,  1.3501962,  1.0373273, ..., -1.3562768, -1.6360753,\n",
       "              -1.7979308], dtype=float32)                                     ,\n",
       "       array([ 0.96715724,  1.3854632 ,  1.6133032 , ..., -1.1245849 ,\n",
       "              -1.3782059 , -1.5465595 ], dtype=float32)               ,\n",
       "       array([ 0.79781646,  0.9409793 ,  0.70317614, ..., -1.5000855 ,\n",
       "              -1.6120543 , -1.6439255 ], dtype=float32)               ,\n",
       "       array([ 0.9697403,  1.0260649,  1.2491024, ..., -1.4390949, -1.4360267,\n",
       "              -1.516563 ], dtype=float32)                                     ,\n",
       "       array([ 1.3766452,  1.3473257,  1.3385692, ..., -1.515266 , -1.5408206,\n",
       "              -1.6801628], dtype=float32)                                     ,\n",
       "       array([ 0.73253393,  0.8724424 ,  0.7248331 , ..., -1.567218  ,\n",
       "              -1.7298324 , -1.8144922 ], dtype=float32)               ,\n",
       "       array([ 1.2468466,  1.1206315,  1.2473   , ..., -1.5027293, -1.5087984,\n",
       "              -1.5900553], dtype=float32)                                     ,\n",
       "       array([ 1.6076941,  1.5938716,  1.5668868, ..., -1.9373738, -1.9195616,\n",
       "              -1.856434 ], dtype=float32)                                     ,\n",
       "       array([ 1.6188761,  1.5189394,  1.2044839, ..., -1.8717417, -1.8813138,\n",
       "              -1.7978891], dtype=float32)                                     ,\n",
       "       array([ 0.4520234,  0.8789703,  1.009094 , ..., -1.3831786, -1.3176415,\n",
       "              -1.4600205], dtype=float32)                                     ,\n",
       "       array([ 1.2111608,  1.2934103,  1.1975282, ..., -1.4542122, -1.6908933,\n",
       "              -1.6817993], dtype=float32)                                     ,\n",
       "       array([ 1.2523694,  1.3517003,  1.5012141, ..., -1.6586103, -1.7625108,\n",
       "              -1.7909229], dtype=float32)                                     ,\n",
       "       array([ 0.78337455,  0.7811543 ,  0.8059981 , ..., -1.6604385 ,\n",
       "              -1.6525898 , -1.6224563 ], dtype=float32)               ,\n",
       "       array([ 1.0998261,  1.1773587,  1.0311651, ..., -1.4632428, -1.7486646,\n",
       "              -1.8958434], dtype=float32)                                     ,\n",
       "       array([ 0.9181351 ,  0.8825631 ,  0.8017884 , ..., -0.5736234 ,\n",
       "              -0.69144243, -0.76706606], dtype=float32)               ,\n",
       "       array([ 0.7126999,  0.9216611,  1.0454429, ..., -1.4417392, -1.4018452,\n",
       "              -1.3952848], dtype=float32)                                     ,\n",
       "       array([ 1.012585 ,  1.132441 ,  1.1921083, ..., -1.428394 , -1.5201501,\n",
       "              -1.573435 ], dtype=float32)                                     ,\n",
       "       array([ 1.3457273,  1.3331833,  1.3283132, ..., -1.4575233, -1.493669 ,\n",
       "              -1.5463855], dtype=float32)                                     ,\n",
       "       array([ 0.46461275,  0.5697636 ,  0.5879229 , ..., -1.6214525 ,\n",
       "              -1.5651219 , -1.5773615 ], dtype=float32)               ,\n",
       "       array([ 1.0441808,  1.003849 ,  0.8491827, ..., -1.2289783, -1.240577 ,\n",
       "              -1.4040843], dtype=float32)                                     ,\n",
       "       array([ 0.7479531 ,  0.8734432 ,  0.9114001 , ..., -0.13877416,\n",
       "               0.22880349, -0.02616937], dtype=float32)               ,\n",
       "       array([ 0.8559219 ,  0.843121  ,  0.81853133, ..., -1.4841642 ,\n",
       "              -1.501022  , -1.6305315 ], dtype=float32)               ,\n",
       "       array([ 0.5933385 ,  0.5609509 ,  0.32825032, ..., -1.2232872 ,\n",
       "              -1.290916  , -1.3414432 ], dtype=float32)               ,\n",
       "       array([ 0.9593699,  0.9968247,  1.3237624, ..., -1.4411023, -1.5503182,\n",
       "              -1.6681532], dtype=float32)                                     ,\n",
       "       array([ 0.80405366,  1.3250738 ,  1.3336844 , ..., -1.6910024 ,\n",
       "              -1.4450911 , -1.1117423 ], dtype=float32)               ,\n",
       "       array([ 1.2947286,  1.2423676,  1.0603149, ..., -0.895666 , -1.0923275,\n",
       "              -1.2483138], dtype=float32)                                     ,\n",
       "       array([ 0.50382197,  0.681168  ,  0.83710706, ..., -0.29244542,\n",
       "              -0.5019711 , -1.0201341 ], dtype=float32)               ,\n",
       "       array([ 0.2358575 ,  0.47619033,  0.50444156, ..., -1.2433306 ,\n",
       "              -1.3323085 , -1.6491815 ], dtype=float32)               ,\n",
       "       array([ 0.6734413 ,  0.86034703,  0.82202727, ..., -0.5947871 ,\n",
       "              -0.80910254, -0.9970302 ], dtype=float32)               ,\n",
       "       array([ 0.8025007,  0.8193141,  0.7152276, ..., -1.7132249, -1.5537575,\n",
       "              -1.481304 ], dtype=float32)                                     ,\n",
       "       array([ 0.51302123,  0.5694107 ,  0.56469667, ..., -1.7346727 ,\n",
       "              -1.6154654 , -1.6166207 ], dtype=float32)               ,\n",
       "       array([ 1.0207672,  1.079524 ,  1.1651082, ..., -1.6542529, -1.634283 ,\n",
       "              -1.475638 ], dtype=float32)                                     ,\n",
       "       array([ 1.3673656,  1.4091061,  1.527893 , ..., -1.379232 , -1.5018163,\n",
       "              -1.6097128], dtype=float32)                                     ,\n",
       "       array([ 1.5278286,  1.3776615,  1.0772134, ..., -1.736739 , -1.6479638,\n",
       "              -1.6852117], dtype=float32)                                     ,\n",
       "       array([ 0.3660467 ,  0.55708045,  0.4126763 , ..., -1.0549399 ,\n",
       "              -1.1957905 , -1.1618245 ], dtype=float32)               ,\n",
       "       array([ 1.3756922,  1.3191586,  1.3743556, ..., -1.3482556, -1.4317843,\n",
       "              -1.5254236], dtype=float32)                                     ,\n",
       "       array([ 1.6123772,  1.5759612,  1.5279989, ..., -1.6928805, -1.6581621,\n",
       "              -1.654295 ], dtype=float32)                                     ,\n",
       "       array([ 1.0453018,  1.1419079,  1.1655421, ..., -1.8390957, -1.9522414,\n",
       "              -1.9173731], dtype=float32)                                     ,\n",
       "       array([ 0.73787516,  0.6712347 ,  0.47680634, ..., -1.0939144 ,\n",
       "              -1.5011083 , -1.5820378 ], dtype=float32)               ,\n",
       "       array([ 0.45678297,  0.6227371 ,  0.5275524 , ..., -1.3696061 ,\n",
       "              -1.5370448 , -1.7200074 ], dtype=float32)               ,\n",
       "       array([ 0.60403293,  0.64668494,  0.51411796, ..., -1.6038611 ,\n",
       "              -1.7549858 , -1.8207843 ], dtype=float32)               ,\n",
       "       array([ 0.76209056,  0.7643658 ,  0.48415405, ..., -0.22345357,\n",
       "              -0.34907216, -0.8337889 ], dtype=float32)               ,\n",
       "       array([ 0.41164285,  0.61351764,  0.55936813, ..., -1.49381   ,\n",
       "              -1.559123  , -1.5801002 ], dtype=float32)               ,\n",
       "       array([ 0.30561218,  0.6259763 ,  0.84365904, ..., -1.7656994 ,\n",
       "              -1.7257662 , -1.6671666 ], dtype=float32)               ,\n",
       "       array([ 0.5678868 ,  0.62911177,  0.6944035 , ..., -1.6134539 ,\n",
       "              -1.608366  , -1.6890965 ], dtype=float32)               ,\n",
       "       array([ 1.320267 ,  1.5444825,  1.5912601, ..., -1.0715569, -1.0122426,\n",
       "              -1.1263775], dtype=float32)                                     ,\n",
       "       array([ 0.6818889 ,  0.7446581 ,  0.52584887, ..., -1.7189984 ,\n",
       "              -1.82604   , -1.8993802 ], dtype=float32)               ,\n",
       "       array([ 0.24546662,  0.22475931,  0.22476262, ..., -0.9935643 ,\n",
       "              -0.9503375 , -1.3146248 ], dtype=float32)               ,\n",
       "       array([ 2.8846817,  2.9804506,  3.0157845, ..., -1.7986811, -1.7431086,\n",
       "              -1.7182814], dtype=float32)                                     ,\n",
       "       array([ 0.64744884,  0.75220263,  0.6360705 , ..., -1.1351053 ,\n",
       "              -1.4542559 , -1.6555654 ], dtype=float32)               ,\n",
       "       array([ 0.9262995 ,  0.9177001 ,  0.85372025, ..., -1.4175867 ,\n",
       "              -1.4893314 , -1.4923956 ], dtype=float32)               ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[np.hstack([np.where(full_patient_IDs==i) for i in train_patient_IDs]).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec949d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "87f59992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('complete_database.h5', 'r') as h5:\n",
    "#     features = h5['features'][:, 0]\n",
    "#     labels = h5['class'][:]\n",
    "#     scores = h5['score'][:]\n",
    "#     folders = h5['folder'][:]\n",
    "#     genders = h5['gender'][:] \n",
    "#     index = h5['index'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145c691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0893403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f27cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c9c6f2",
   "metadata": {},
   "source": [
    "# test audio loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3428e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b7aa150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionDataset(Dataset):\n",
    "    '''create a training, develop, or test dataset\n",
    "       and load the participant features if it's called\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 mode,\n",
    "                 transform=None):\n",
    "        super(DepressionDataset, self).__init__()\n",
    "\n",
    "        # only train, develop, test dataset allow\n",
    "        assert mode in [\"train\", \"validation\", \"test\"], \\\n",
    "            \"Argument --mode could only be ['train', 'validation', 'test']\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train_data_path = os.path.join(self.root_dir, 'train_split_Depression_AVEC2017.csv')\n",
    "        self.valid_data_path = os.path.join(self.root_dir, 'dev_split_Depression_AVEC2017.csv')\n",
    "        self.test_data_path = os.path.join(self.root_dir, 'full_test_split.csv')\n",
    "        # load sent2vec model for converting text file to 2D array\n",
    "#         self.sent2vec = SentenceTransformer('all-mpnet-base-v2')  # output dimension 768\n",
    "\n",
    "        # get full patient ID list\n",
    "        train_patient_IDs = pd.read_csv(self.train_data_path)['Participant_ID'].to_numpy()\n",
    "        valid_patient_IDs = pd.read_csv(self.valid_data_path)['Participant_ID'].to_numpy()\n",
    "        test_patient_IDs = pd.read_csv(self.test_data_path)['Participant_ID'].to_numpy()\n",
    "        self.full_patient_IDs = np.sort(np.hstack([train_patient_IDs, valid_patient_IDs, test_patient_IDs]))\n",
    "    \n",
    "        # load training data # 107 sessions\n",
    "        if self.mode == \"train\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.train_data_path))\n",
    "            # store ground truth\n",
    "            ####################################################################################################\n",
    "            # self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.patientIDs = np.array([303, 321, 362, 363, 426])  # for debugging on my laptop\n",
    "            ####################################################################################################\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load development data # 35 sessions\n",
    "        if self.mode == \"validation\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.valid_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ8_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ8_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            self.phq_subscores_gt = self.data_df.iloc[:, 4:].to_numpy()\n",
    "\n",
    "        # load test data # 47 sessions\n",
    "        if self.mode == \"test\":\n",
    "            # pre-checking and cleaning the data\n",
    "            self.data_df = self.pre_check(pd.read_csv(self.test_data_path))\n",
    "            # store ground truth\n",
    "            self.patientIDs = self.data_df['Participant_ID'].to_numpy()\n",
    "            self.phq_binay_gt = self.data_df['PHQ_Binary'].to_numpy()\n",
    "            self.phq_score_gt = self.data_df['PHQ_Score'].to_numpy()\n",
    "            self.gender_gt = self.data_df['Gender'].to_numpy()\n",
    "            # subscores in test data are not provided, thus we initialize it with 0 to avoid error for DataLoader\n",
    "            self.phq_subscores_gt = np.zeros((self.patientIDs.shape[0],  8))\n",
    "                \n",
    "        # get sampler\n",
    "        target = self.phq_binay_gt  # np.array([0,1,1,0,1])  # self.phq_binay_gt\n",
    "        class_sample_count = np.unique(target, return_counts=True)[1]\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = weight[target]\n",
    "        samples_weight = torch.from_numpy(samples_weight).double()\n",
    "        self.sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        \n",
    "            \n",
    "    def pre_check(self, data):\n",
    "        '''\n",
    "        Basic cleaning process to make sure no missing value\n",
    "        and that the sum of each PHQ subscore equals to PHQ score\n",
    "        Argument:\n",
    "            data: numpy array\n",
    "        Return:\n",
    "            data: numpy array with type \"int\"\n",
    "        '''\n",
    "        # make sure no NaN, Inf, -Inf\n",
    "        if data.isin([np.nan, np.inf, -np.inf]).any(1).sum():\n",
    "            print('Replacing NaN, Inf, or -Inf ...')\n",
    "            data = data.replace([np.inf, -np.inf, np.nan], 0)  # .astype('int')\n",
    "        else:\n",
    "            data = data  # .astype('int')\n",
    "\n",
    "        # compare the sum of each PHQ subscore to PHQ score\n",
    "        unequal = data.iloc[:, 4:].sum(axis=1) != data.iloc[:, 2]\n",
    "        if unequal.any() and self.mode != 'test':\n",
    "            lines = np.where(unequal)\n",
    "            raise ValueError((\"The sum of each PHQ subscore at line {} \"\n",
    "                              \"is unequal to the PHQ score\").format(lines[0]))\n",
    "            \n",
    "        # check whether the PHQ binary is correctly converted based on PHQ score \n",
    "        phq_binary = data.iloc[:, 1].to_numpy()\n",
    "        phq_score = data.iloc[:, 2].to_numpy()\n",
    "        phq_converted_binary = np.where(phq_score > 9, 1, 0)\n",
    "        if (phq_converted_binary != phq_binary).any():\n",
    "            where = np.where(phq_converted_binary != phq_binary)\n",
    "            data.iloc[where, 1] = phq_converted_binary[where]\n",
    "\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patientIDs)\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.patientIDs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Essentional function for creating dataset in PyTorch, which will automatically be\n",
    "        called in Dataloader and load all the extracted features of the patient in the Batch\n",
    "        based on the index of self.patientIDs\n",
    "        Argument:\n",
    "            idx: int, index of the patient ID in self.patientIDs\n",
    "        Return:\n",
    "            session: dict, contains all the extracted features and ground truth of a patient/session\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # get the patient session path\n",
    "        session_num = self.patientIDs[idx]\n",
    "        session_path = os.path.join(self.root_dir, '{}_P'.format(session_num))\n",
    "\n",
    "        # TODO: if other feature is needed, add more in the following part...\n",
    "\n",
    "#         # get audio feature path\n",
    "#         audio_path = os.path.join(session_path, '{}_AUDIO.wav'.format(session_num))\n",
    "        \n",
    "        # load audio file\n",
    "        audio_root = 'D:/DAIC-WOZ_dataset/Audio/logmel_snv_exp'\n",
    "        audio_path = os.path.join(audio_root, 'complete_database.h5')\n",
    "        # audio feature extration\n",
    "        idx_in_whole_dataset = np.where(full_patient_IDs==self.patientIDs[idx])[0][0]\n",
    "        with h5py.File(audio_path, 'r') as h5:\n",
    "            audio_feature = h5['features'][:, 0][idx_in_whole_dataset]\n",
    "        audio = np.transpose(audio_feature.reshape(80, -1))\n",
    "#         audio, self.audio_parameters = self.load_audio(audio_path)\n",
    "        \n",
    "        # summary\n",
    "        session = {'patientID': session_num,\n",
    "                   'session_path': session_path,\n",
    "                   'audio': audio,\n",
    "                   'phq_score_gt': self.phq_score_gt[idx],\n",
    "                   'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "                   'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "                   'gender_gt': self.gender_gt[idx]}\n",
    "        \n",
    "#         # get all features path of the session\n",
    "#         facial_landmarks_path = os.path.join(session_path, '{}_CLNF_features3D.txt'.format(session_num))\n",
    "#         gaze_direction_path = os.path.join(session_path, '{}_CLNF_gaze.txt'.format(session_num))\n",
    "#         audio_path = os.path.join(session_path, '{}_AUDIO.wav'.format(session_num))\n",
    "#         text_path = os.path.join(session_path, '{}_TRANSCRIPT.csv'.format(session_num))\n",
    "        \n",
    "#         # facial feature\n",
    "#         facial_landmarks = self.load_facial_landmarks(facial_landmarks_path)\n",
    "#         # gaze direction feature\n",
    "#         gaze_direction = pd.read_csv(gaze_direction_path).iloc[:, 4:].to_numpy()\n",
    "#         # audion feature, but constrain the rows based to match the shape of landmarks/gaze_sample\n",
    "#         audio, self.audio_parameters = self.load_audio(audio_path)\n",
    "#         audio = audio[:facial_landmarks.shape[0]]\n",
    "#         # text feature\n",
    "#         self.text_feature = self.load_sent2vec(text_path, speaker='Participant')\n",
    "#         sentence_embedding = self.text_feature['sentence_embeddings']\n",
    "\n",
    "#         # summary\n",
    "#         session = {'patientID': session_num,\n",
    "#                    'session_path': session_path,\n",
    "#                    'facial_landmarks': facial_landmarks,\n",
    "#                    'gaze_direction': gaze_direction, \n",
    "#                    'audio': audio,\n",
    "#                    'sentence_embeddings': sentence_embedding,\n",
    "#                    'phq_score_gt': self.phq_score_gt[idx],\n",
    "#                    'phq_binay_gt': self.phq_binay_gt[idx],\n",
    "#                    'phq_subscores_gt': self.phq_subscores_gt[idx],\n",
    "#                    'gender_gt': self.gender_gt[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            session = self.transform(session)\n",
    "\n",
    "        return session\n",
    "    \n",
    "    def load_audio(self, audio_path, \n",
    "                   spectro_type='mel_spectrogram', \n",
    "                   frame_size = 2048,\n",
    "                   hop_size = 533,\n",
    "                   sample_rate = 16000,\n",
    "                   num_mel_bands = 80,\n",
    "                   preprocess=True):\n",
    "        '''\n",
    "        Standard method of loading audio and extracting audio features\n",
    "        with Short-Time Fourier Transform by utilizing librosa library\n",
    "        Arguments:\n",
    "            audio_path: string, absolute path to audio file\n",
    "            preprocess: boolean, whether normalize the data\n",
    "        Return:\n",
    "            audio_feature: 2D numpy.ndarray, extracted audio feature (spectra) in dB\n",
    "        '''\n",
    "        # only spectrogram and mel_spectrogram are allow\n",
    "        assert spectro_type in ['spectrogram', 'mel_spectrogram'],\\\n",
    "            \"Argument --spectro could only be ['spectrogram', 'mel_spectrogram']\"\n",
    "        \n",
    "        # parameter setting for Short-Time Fourier Transform\n",
    "        audio_parameters = {'spectro_type': spectro_type,\n",
    "                            'sample_rate': sample_rate,\n",
    "                            'frame_size': frame_size,\n",
    "                            'hop_size': hop_size, \n",
    "                            'num_mel_bands': num_mel_bands}\n",
    "        \n",
    "        # load audio file with librosa\n",
    "        sampled_values, sr = librosa.load(audio_path, sr=audio_parameters['sample_rate'])  \n",
    "        ''' According to documnet sample rate is 16kHz\n",
    "            sampled_values: audio sampled values of time series\n",
    "            sr: sampling rate of audio\n",
    "        '''\n",
    "        \n",
    "        # extracting features\n",
    "        if spectro_type == 'spectrogram':\n",
    "            # use Short-Time Fourier Transform, return complex-valued matrix STFT coefficients\n",
    "            extracted_values = librosa.stft(sampled_values, \n",
    "                                            n_fft=audio_parameters['frame_size'], \n",
    "                                            hop_length=audio_parameters['hop_size'])\n",
    "            # calculating the spectrogram\n",
    "            extracted_values = np.abs(extracted_values) ** 2\n",
    "            # print(\"Shape of the extracted features in dB: {}\".format(extracted_values.shape))\n",
    "            \n",
    "        elif spectro_type == 'mel_spectrogram':\n",
    "            # get the Mel filter banks\n",
    "            self.filter_banks = librosa.filters.mel(n_fft=audio_parameters['frame_size'], \n",
    "                                                    sr=audio_parameters['sample_rate'], \n",
    "                                                    n_mels=audio_parameters['num_mel_bands'])\n",
    "            # extract the mel spectrograom\n",
    "            extracted_values = librosa.feature.melspectrogram(sampled_values, \n",
    "                                                              sr=audio_parameters['sample_rate'], \n",
    "                                                              n_fft=audio_parameters['frame_size'], \n",
    "                                                              hop_length=audio_parameters['hop_size'], \n",
    "                                                              n_mels=audio_parameters['num_mel_bands'])\n",
    "            # print(\"Shape of the extracted_values in dB: {}\".format(extracted_values.shape))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"The given value of spectro_type is not supported\\n\"\n",
    "                             \"'spectro_type' could only be 'spectrogram' or 'mel_spectrogram'\")\n",
    "        \n",
    "        # convert amplitude to DBs\n",
    "        # transpose the result so that the rows corresponds to time and column to frequence\n",
    "        audio_feature = np.transpose(librosa.power_to_db(extracted_values))\n",
    "        # print(\"Shape of the final extracted audio feature (spectra) in dB: {}\".format(audio_feature.shape))\n",
    "        \n",
    "        if preprocess:\n",
    "            audio_feature = audio_minmax_scaler(audio_feature)\n",
    "            \n",
    "        return audio_feature, audio_parameters\n",
    "    \n",
    "\n",
    "class Padding(object):\n",
    "    ''' pad zero to each feature matrix so that they all have the same size '''\n",
    "\n",
    "    def __init__(self, audio_output_size=(58989, 80)):\n",
    "        super(Padding, self).__init__()\n",
    "        '''\n",
    "        Each output size could be 'int' or 'tuple'. \n",
    "        Integer would be the number of desired rows\n",
    "        and Tuple would be the desired 2D array size.\n",
    "\n",
    "        Here is recommended to keep the number of columns \n",
    "        as they are and only set the number of rows with int\n",
    "\n",
    "        To find the maximum length of rows, please use the \n",
    "        'find_max_length' function in utils to search through. \n",
    "\n",
    "        The value 386 are the maximum length in our case.\n",
    "        '''\n",
    "        assert isinstance(audio_output_size, (int, tuple))\n",
    "        self.audio_output_size = audio_output_size\n",
    "\n",
    "        \n",
    "    def __call__(self, session):\n",
    "        audio = session['audio']\n",
    "        \n",
    "        # audio padding along heigh dimension\n",
    "        if isinstance(self.audio_output_size, int):\n",
    "            h, w = audio.shape\n",
    "            new_h = self.audio_output_size if h > self.audio_output_size else h\n",
    "            padded_audio = np.zeros((self.audio_output_size, w))\n",
    "            padded_audio[:new_h, :w] = audio[:new_h, :w]\n",
    "        \n",
    "        # audio padding along both heigh and width dimension\n",
    "        else:\n",
    "            h, w = audio.shape\n",
    "            new_h = self.audio_output_size[0] if h > self.audio_output_size[0] else h\n",
    "            new_w = self.audio_output_size[1] if w > self.audio_output_size[1] else w\n",
    "            padded_audio = np.zeros(self.audio_output_size)\n",
    "            padded_audio[:new_h, :new_w] = audio[:new_h, :new_w]\n",
    "\n",
    "        # summary\n",
    "        padded_session = {'patientID': session['patientID'],\n",
    "                          'session_path': session['session_path'],\n",
    "                          'audio': padded_audio,\n",
    "                          'phq_score_gt': session['phq_score_gt'],\n",
    "                          'phq_binay_gt': session['phq_binay_gt'],\n",
    "                          'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                          'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return padded_session\n",
    "\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int),  Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size=(256, 256)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        audio = session['audio']\n",
    "\n",
    "        h, w = audio.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        rescaled_audio = transform.resize(audio, (new_h, new_w))\n",
    "\n",
    "        # summary\n",
    "        rescaled_session = {'patientID': session['patientID'],\n",
    "                            'session_path': session['session_path'],\n",
    "                            'audio': rescaled_audio,\n",
    "                            'phq_score_gt': session['phq_score_gt'],\n",
    "                            'phq_binay_gt': session['phq_binay_gt'],\n",
    "                            'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                            'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return rescaled_session\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "    Arguments:\n",
    "        output_size:(tuple or int), Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size=(224, 224)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, session):\n",
    "        audio = session['audio']\n",
    "\n",
    "        h, w = sentence_embeddings.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        cropped_audio = audio[top:top + new_h, left:left + new_w]\n",
    "\n",
    "        # summary\n",
    "        cropped_session = {'patientID': session['patientID'],\n",
    "                           'session_path': session['session_path'],\n",
    "                           'audio': cropped_audio,\n",
    "                           'phq_score_gt': session['phq_score_gt'],\n",
    "                           'phq_binay_gt': session['phq_binay_gt'],\n",
    "                           'phq_subscores_gt': session['phq_subscores_gt'],\n",
    "                           'gender_gt': session['gender_gt']}\n",
    "\n",
    "        return cropped_session\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors or np.int to torch.tensor.\"\"\"\n",
    "\n",
    "    def __call__(self, session):\n",
    "        converted_session = {'patientID': session['patientID'],\n",
    "                             'session_path': session['session_path'],\n",
    "                             'audio': torch.from_numpy(session['audio']).type(torch.FloatTensor),\n",
    "                             'phq_score_gt': torch.tensor(session['phq_score_gt']).type(torch.FloatTensor),\n",
    "                             'phq_binay_gt': torch.tensor(session['phq_binay_gt']).type(torch.FloatTensor),\n",
    "                             'phq_subscores_gt': torch.from_numpy(session['phq_subscores_gt']).type(torch.FloatTensor),\n",
    "                             'gender_gt': torch.tensor(session['gender_gt']).type(torch.FloatTensor)}\n",
    "\n",
    "        return converted_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "80aa962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number:  0 , audio:  torch.Size([2, 49109, 80])\n",
      "=================================\n",
      "Batch number:  1 , audio:  torch.Size([2, 49109, 80])\n",
      "=================================\n",
      "Batch number:  2 , audio:  torch.Size([1, 49109, 80])\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import transforms\n",
    "\n",
    "    # sys.path.append('C:/Users/denni/Documents/KIT Studium/Bachelorarbeit')\n",
    "    root_dir = 'C:/Users/denni/Documents/KIT Studium/Bachelorarbeit'\n",
    "\n",
    "    # test 3: try to load the dataset with DataLoader\n",
    "    transformed_dataset = DepressionDataset(os.path.join(root_dir, 'DAIC-WOZ Dataset'), 'train', \n",
    "                                            transform=transforms.Compose([Padding((49109, 80)), ToTensor()]))\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(transformed_dataset,\n",
    "                            batch_size=2,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "    # iterate through batches\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print('Batch number: ', i_batch, ', audio: ', sample_batched['audio'].size())\n",
    "        print('=================================')\n",
    "        if i_batch == 1:\n",
    "            test_batch = sample_batched\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
