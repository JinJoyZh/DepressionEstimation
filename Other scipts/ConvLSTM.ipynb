{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6300b4f8",
   "metadata": {},
   "source": [
    "# Convolution_LSTM_pytorch\n",
    "\n",
    "https://github.com/automan000/Convolutional_LSTM_PyTorch\n",
    "\n",
    "```\n",
    "clstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64], kernel_size=5, step=9, effective_step=[2, 4, 8])\n",
    "lstm_outputs = clstm(cnn_features)\n",
    "hidden_states = lstm_outputs[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fadcb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ca9636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "            self.Wcf = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "            self.Wco = nn.Parameter(torch.zeros(1, hidden, shape[0], shape[1])).cuda()\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda(),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(self.step):\n",
    "            x = input\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                if step == 0:\n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             shape=(height, width))\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            # only record effective steps\n",
    "            if step in self.effective_step:\n",
    "                outputs.append(x)\n",
    "\n",
    "        return outputs, (x, new_c)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # gradient check\n",
    "#     convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size=3, step=5,\n",
    "#                         effective_step=[4]).cuda()\n",
    "#     loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#     input = Variable(torch.randn(1, 512, 64, 32)).cuda()\n",
    "#     target = Variable(torch.randn(1, 32, 64, 32)).double().cuda()\n",
    "\n",
    "#     output = convlstm(input)\n",
    "#     output = output[0][0].double()\n",
    "#     res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e30ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm = ConvLSTM(input_channels=100, hidden_channels=[128, 64, 64], kernel_size=5, step=9, effective_step=[2, 4, 8])\n",
    "clstm = clstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec1a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 100, 25, 3, requires_grad=True).cuda()\n",
    "lstm_outputs = clstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1e610d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (x, new_c) = lstm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47ec2461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)  # == len(effective_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d970059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 25, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape # == [N, hidden_channels[-1], input.size(2), input.size(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f31f0810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 25, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # final output after all LSTM Layers, shape: [N, hidden_channels[-1], input.size(2), input.size(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87a81f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 25, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c2e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d635384",
   "metadata": {},
   "source": [
    "# Video ConvLSTM_pytorch\n",
    "\n",
    "https://github.com/ndrplz/ConvLSTM_pytorch\n",
    "\n",
    "```\n",
    "model = ConvLSTM(input_dim=channels,\n",
    "                 hidden_dim=[64, 64, 128],\n",
    "                 kernel_size=(3, 3),\n",
    "                 num_layers=3,\n",
    "                 batch_first=True\n",
    "                 bias=True,\n",
    "                 return_all_layers=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f30ef84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "275ad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "830dc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 100\n",
    "model = ConvLSTM(input_dim=channels,\n",
    "                 hidden_dim=[64, 64, 128],\n",
    "                 kernel_size=(3, 3),\n",
    "                 num_layers=3,\n",
    "                 batch_first=True,\n",
    "                 bias=True,\n",
    "                 return_all_layers=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1266a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 1, 100, 25, 4, requires_grad=True).cuda()  # B, T, C, H, W\n",
    "layer_output_list, last_state_list = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bcc02c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 25, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output_list[0].shape  # [B, T, hidden_dim[-1], H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91cbe60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_state_list[0])  # == 2, return of ConvLSTMCell: h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7102050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 25, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_state_list[0][0].shape  # last state of h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5af29cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 25, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_state_list[0][1].shape # last state of c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d982b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5753d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449056b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd52beb9",
   "metadata": {},
   "source": [
    "# Deep Audio Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a11306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer.\n",
    "    Ref: He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing\n",
    "    human-level performance on imagenet classification.\" Proceedings of the\n",
    "    IEEE international conference on computer vision. 2015.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The current layer of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    if layer.weight.ndimension() == 4:\n",
    "        (n_out, n_in, height, width) = layer.weight.size()\n",
    "        n = n_in * height * width\n",
    "    elif layer.weight.ndimension() == 3:\n",
    "        (n_out, n_in, height) = layer.weight.size()\n",
    "        n = n_in * height\n",
    "    elif layer.weight.ndimension() == 2:\n",
    "        (n_out, n) = layer.weight.size()\n",
    "\n",
    "    std = math.sqrt(2. / n)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_lstm(layer):\n",
    "    \"\"\"\n",
    "    Initialises the hidden layers in the LSTM - H0 and C0.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The LSTM layer\n",
    "    \"\"\"\n",
    "    n_i1, n_i2 = layer.weight_ih_l0.size()\n",
    "    n_i = n_i1 * n_i2\n",
    "\n",
    "    std = math.sqrt(2. / n_i)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight_ih_l0.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias_ih_l0 is not None:\n",
    "        layer.bias_ih_l0.data.fill_(0.)\n",
    "\n",
    "    n_h1, n_h2 = layer.weight_hh_l0.size()\n",
    "    n_h = n_h1 * n_h2\n",
    "\n",
    "    std = math.sqrt(2. / n_h)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight_hh_l0.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias_hh_l0 is not None:\n",
    "        layer.bias_hh_l0.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_att_layer(layer):\n",
    "    \"\"\"\n",
    "    Initilise the weights and bias of the attention layer to 1 and 0\n",
    "    respectively. This is because the first iteration through the attention\n",
    "    mechanism should weight each time step equally.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The current layer of the neural network\n",
    "    \"\"\"\n",
    "    layer.weight.data.fill_(1.)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"\n",
    "    Initialize a Batchnorm layer.\n",
    "\n",
    "    Input\n",
    "        bn: torch.Tensor - The batch normalisation layer\n",
    "    \"\"\"\n",
    "\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "class ConvBlock1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a 1D convolutional layer. This includes the\n",
    "    convolutional filter but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, pad,\n",
    "                 normalisation, dil=1):\n",
    "        super(ConvBlock1d, self).__init__()\n",
    "        self.norm = normalisation\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel,\n",
    "                               stride=stride,\n",
    "                               padding=pad,\n",
    "                               dilation=dil)\n",
    "        if self.norm == 'bn':\n",
    "            self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        elif self.norm == 'wn':\n",
    "            self.conv1 = nn.utils.weight_norm(self.conv1, name='weight')\n",
    "        else:\n",
    "            self.conv1 = self.conv1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the convolutional filter\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.norm == 'bn':\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "        else:\n",
    "            x = self.relu(self.conv1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a 2D convolutional layer. This includes the\n",
    "    convolutional filter but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, pad,\n",
    "                 normalisation, att=None):\n",
    "        super(ConvBlock2d, self).__init__()\n",
    "        self.norm = normalisation\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel,\n",
    "                               stride=stride,\n",
    "                               padding=pad)\n",
    "        if self.norm == 'bn':\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        elif self.norm == 'wn':\n",
    "            self.conv1 = nn.utils.weight_norm(self.conv1, name='weight')\n",
    "        else:\n",
    "            self.conv1 = self.conv1\n",
    "        self.att = att\n",
    "        if not self.att:\n",
    "            self.act = nn.ReLU()\n",
    "        else:\n",
    "            self.norm = None\n",
    "            if self.att == 'softmax':\n",
    "                self.act = nn.Softmax(dim=-1)\n",
    "            elif self.att == 'global':\n",
    "                self.act = None\n",
    "            else:\n",
    "                self.act = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        if self.att:\n",
    "            init_att_layer(self.conv1)\n",
    "        else:\n",
    "            init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the convolutional filter\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.att:\n",
    "            x = self.conv1(x)\n",
    "            if self.act():\n",
    "                x = self.act(x)\n",
    "        else:\n",
    "            if self.norm == 'bn':\n",
    "                x = self.act(self.bn1(self.conv1(x)))\n",
    "            else:\n",
    "                x = self.act(self.conv1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a fully-connected layer. This includes the\n",
    "    hidden layers but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, activation, normalisation,\n",
    "                 att=None):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.att = att\n",
    "        self.norm = normalisation\n",
    "        self.fc = nn.Linear(in_features=in_channels,\n",
    "                            out_features=out_channels)\n",
    "        if activation == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "            self.norm = None\n",
    "        elif activation == 'softmax':\n",
    "            self.act = nn.Softmax(dim=-1)\n",
    "            self.norm = None\n",
    "        elif activation == 'global':\n",
    "            self.act = None\n",
    "            self.norm = None\n",
    "        else:\n",
    "            self.act = nn.ReLU()\n",
    "            if self.norm == 'bn':\n",
    "                self.bnf = nn.BatchNorm1d(out_channels)\n",
    "            elif self.norm == 'wn':\n",
    "                self.wnf = nn.utils.weight_norm(self.fc, name='weight')\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        if self.att:\n",
    "            init_att_layer(self.fc)\n",
    "        else:\n",
    "            init_layer(self.fc)\n",
    "        if self.norm == 'bn':\n",
    "            init_bn(self.bnf)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the fully-connected layer\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.norm is not None:\n",
    "            if self.norm == 'bn':\n",
    "                x = self.act(self.bnf(self.fc(x)))\n",
    "            else:\n",
    "                x = self.act(self.wnf(x))\n",
    "        else:\n",
    "            if self.att:\n",
    "                if self.act:\n",
    "                    x = self.act(self.fc(x))\n",
    "                else:\n",
    "                    x = self.fc(x)\n",
    "            else:\n",
    "                if self.act:\n",
    "                    x = self.act(self.fc(x))\n",
    "                else:\n",
    "                    x = self.fc(x)        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# def lstm_with_attention(net_params):\n",
    "#     if 'LSTM_1' in net_params:\n",
    "#         arguments = net_params['LSTM_1']\n",
    "#     else:\n",
    "#         arguments = net_params['GRU_1']\n",
    "#     if 'ATTENTION_1' in net_params and 'ATTENTION_Global' not in net_params:\n",
    "#         if arguments[-1]:\n",
    "#             return 'forward'\n",
    "#         else:\n",
    "#             return 'whole'\n",
    "#     if 'ATTENTION_1' in net_params and 'ATTENTION_Global' in net_params:\n",
    "#         if arguments[-1]:\n",
    "#             return 'forward'\n",
    "#         else:\n",
    "#             return 'whole'\n",
    "#     if 'ATTENTION_1' not in net_params and 'ATTENTION_Global' in net_params:\n",
    "#         if arguments[-1]:\n",
    "#             return 'forward_only'\n",
    "#         else:\n",
    "#             return 'forward_only'\n",
    "\n",
    "\n",
    "def reshape_x(x):\n",
    "    \"\"\"\n",
    "    Reshapes the input 'x' if there is a dimension of length 1\n",
    "\n",
    "    Input:\n",
    "        x: torch.Tensor - The input\n",
    "\n",
    "    Output:\n",
    "        x: torch.Tensor - Reshaped\n",
    "    \"\"\"\n",
    "    dims = x.dim()\n",
    "    if x.shape[1] == 1 and x.shape[2] == 1 and x.shape[3] == 1:\n",
    "        x = torch.reshape(x, (x.shape[0], 1))\n",
    "    elif dims == 4:\n",
    "        first, second, third, fourth = x.shape\n",
    "        if second == 1:\n",
    "            x = torch.reshape(x, (first, third, fourth))\n",
    "        elif third == 1:\n",
    "            x = torch.reshape(x, (first, second, fourth))\n",
    "        else:\n",
    "            x = torch.reshape(x, (first, second, third))\n",
    "    elif dims == 3:\n",
    "        first, second, third = x.shape\n",
    "        if second == 1:\n",
    "            x = torch.reshape(x, (first, third))\n",
    "        elif third == 1:\n",
    "            x = torch.reshape(x, (first, second))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ConvLSTM_2D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv2D_hidden, conv1D_hidden, \n",
    "                 lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(ConvLSTM_2D, self).__init__()\n",
    "        self.conv2D = ConvBlock2d(in_channels=input_dim,\n",
    "                                  out_channels=conv2D_hidden,\n",
    "                                  kernel=(68, 5),\n",
    "                                  stride=(1, 1),\n",
    "                                  pad=(33, 2),\n",
    "                                  normalisation='bn')\n",
    "        self.conv1D = ConvBlock1d(in_channels=22*conv2D_hidden,     # (((num_kp + 2*P) - K)//S + 1) // max_pool * conv2D_hidden\n",
    "                                  out_channels=conv1D_hidden,       # 512\n",
    "                                  kernel=11,\n",
    "                                  stride=1,\n",
    "                                  pad=5,\n",
    "                                  normalisation='bn')\n",
    "        self.pool2D = nn.MaxPool2d(kernel_size=3,\n",
    "                                   stride=3,\n",
    "                                   padding=0)\n",
    "        self.pool1D = nn.MaxPool1d(kernel_size=5,\n",
    "                                   stride=5,\n",
    "                                   padding=0)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=conv1D_hidden,\n",
    "                            hidden_size=lstm_hidden,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.attention_layer = nn.Sequential(nn.Linear(lstm_hidden, lstm_hidden),\n",
    "                                             nn.ReLU(inplace=True))\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,\n",
    "                                 out_channels=output_dim,\n",
    "                                 activation=activation,\n",
    "                                 normalisation=norm)\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        m = nn.Tanh()(h)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, V, T = x.shape\n",
    "        x = self.conv2D(x)  # output x: (N, conv2D_hidden, 67, 1800)\n",
    "        x = self.pool2D(x)  # output x: (N, conv2D_hidden, 67//3, 1800//3)  2, 16, 22, 600\n",
    "        N, C, V, T = x.shape\n",
    "        x = x.view(N, C * V, T)\n",
    "        x = self.conv1D(x)  # output x: (N, conv1D_hidden, 600)\n",
    "        x = self.pool1D(x)  # output x: (N, conv1D_hidden, 600//5)\n",
    "        x = self.drop(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, (final_hidden_state, final_cell_state) = self.lstm(x)               # output x: (N, 120, lstm_hidden)\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2).contiguous()  # output final_hidden_state: (N, num_layer*num_direction, lstm_hidden)\n",
    "        x = self.attention_net_with_w(x, final_hidden_state)                   # output x: (N, lstm_hidden)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class ConvLSTM_1D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_hidden,\n",
    "                 lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(ConvLSTM_1D, self).__init__()\n",
    "        self.conv2D = ConvBlock2d(in_channels=input_dim,\n",
    "                                  out_channels=conv_hidden,\n",
    "                                  kernel=(68, 11),\n",
    "                                  stride=(1, 1),\n",
    "                                  pad=(0, 5),\n",
    "                                  normalisation='bn')\n",
    "        self.pool1D = nn.MaxPool1d(kernel_size=5,\n",
    "                                   stride=5,\n",
    "                                   padding=0)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=conv_hidden,\n",
    "                            hidden_size=lstm_hidden,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.attention_layer = nn.Sequential(nn.Linear(lstm_hidden, lstm_hidden),\n",
    "                                             nn.ReLU(inplace=True))\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,\n",
    "                                 out_channels=output_dim,\n",
    "                                 activation=activation,\n",
    "                                 normalisation=norm)\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        m = nn.Tanh()(h)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, V, T = x.shape\n",
    "        x = self.conv2D(x)  # output x: (N, conv_hidden, 1, 1800)\n",
    "        x = self.pool1D(x.squeeze())  # output x: (N, conv_hidden, 1800//5)\n",
    "        x = self.drop(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, (final_hidden_state, final_cell_state) = self.lstm(x)               # output x: (N, 120, lstm_hidden)\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2).contiguous()  # output final_hidden_state: (N, num_layer*num_direction, lstm_hidden)\n",
    "        x = self.attention_net_with_w(x, final_hidden_state)                   # output x: (N, lstm_hidden)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomMel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_hidden, lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(CustomMel, self).__init__()\n",
    "        self.conv = ConvBlock1d(in_channels=input_dim,      # 80\n",
    "                                out_channels=conv_hidden,   # 128\n",
    "                                kernel=3,\n",
    "                                stride=1,\n",
    "                                pad=1,\n",
    "                                normalisation='bn')         # ['bn', 'wn', else]\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3,\n",
    "                                 stride=3,\n",
    "                                 padding=0)\n",
    "        self.drop = nn.Dropout(dropout)                     # 0.2\n",
    "        self.lstm = nn.LSTM(input_size=conv_hidden,         # 128\n",
    "                            hidden_size=lstm_hidden,        # 128\n",
    "                            num_layers=num_layers,          # 2\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,   # 128\n",
    "                                 out_channels=output_dim,   # 2\n",
    "                                 activation=activation,     # ['sigmoid', 'softmax', 'global', else]\n",
    "                                 normalisation=norm)        # ['bn', 'wn']: nn.BatchNorm1d, nn.utils.weight_norm                \n",
    "\n",
    "    def forward(self, net_input):\n",
    "        x = net_input\n",
    "        batch, freq, width = x.shape\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x, _ = self.lstm(x)                                 # output shape: (batch, width//stride(pool), lstm_hidden) 5x600x128\n",
    "        x = self.fc(x[:, -1, :].reshape(batch, -1))         # output shape: (batch, output_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomRaw(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_hidden, lstm_hidden, num_layers, activation, dropout):\n",
    "        super(CustomRaw).__init__()\n",
    "        # x = [(in + (2*pad) - (kernel-1) - 1) / stride] + 1\n",
    "        self.conv1 = ConvBlock1d(in_channels=input_dim,     # 1\n",
    "                                 out_channels=conv_hidden,  # 128\n",
    "                                 kernel=1024,               # 512\n",
    "                                 stride=512,                # 512\n",
    "                                 pad=0,\n",
    "                                 dil=1,\n",
    "                                 normalisation='bn')\n",
    "\n",
    "        self.conv2 = ConvBlock1d(in_channels=conv_hidden,   # 128\n",
    "                                 out_channels=conv_hidden,  # 128\n",
    "                                 kernel=3,\n",
    "                                 stride=1,\n",
    "                                 pad=1,\n",
    "                                 normalisation='bn')\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3,\n",
    "                                  stride=3,\n",
    "                                  padding=0)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=conv_hidden,         # 128\n",
    "                            hidden_size=lstm_hidden,        # 128\n",
    "                            num_layers=num_layers,          # 2\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,   # 128\n",
    "                                 out_channels=output_dim,   # 2\n",
    "                                 activation=activation,     # ['sigmoid', 'softmax', 'global', else]\n",
    "                                 normalisation=None)\n",
    "\n",
    "    def forward(self, net_input):\n",
    "        x = net_input\n",
    "        batch, freq, width = x.shape\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :].reshape(batch, -1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc28e59",
   "metadata": {},
   "source": [
    "## test conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8113b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 80\n",
    "conv_hidden = 128\n",
    "kernel = 3\n",
    "stride = 1\n",
    "pad = 1\n",
    "normalisation = 'bn'\n",
    "\n",
    "input_1D = torch.randn(2, 80, 300)   # (batch_size, freq(height), width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "359e962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1D = ConvBlock1d(in_channels=input_dim,      # 80\n",
    "                      out_channels=conv_hidden,   # 128\n",
    "                      kernel=kernel,\n",
    "                      stride=stride,\n",
    "                      pad=pad,\n",
    "                      normalisation=normalisation)         # ['bn', 'wn', else]\n",
    "conv_1D = conv_1D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e3c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1D = conv_1D(input_1D.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "861a9d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 300])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1D.shape  # (2, 80, 300) -> (2, conv_hidden, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "031653d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_1D = nn.MaxPool1d(kernel_size=3, stride=3, padding=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8dc785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_result_1D = pool_1D(result_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9c84d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 100])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_result_1D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87845a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 128])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(pool_result_1D, 1, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0d6f8",
   "metadata": {},
   "source": [
    "## test conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98368d32",
   "metadata": {},
   "source": [
    "### conv2D + pool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2c0fb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "conv_hidden = 16\n",
    "kernel = (68, 11)\n",
    "stride= 1\n",
    "pad = (0, 5)\n",
    "normalisation = 'bn'\n",
    "\n",
    "input_2D = torch.randn(2, 4, 68, 1800)   # (batch_size, channel, heigh(num_KPs), width(time series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2c18791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2D = ConvBlock2d(in_channels=input_dim,      \n",
    "                      out_channels=conv_hidden,   \n",
    "                      kernel=kernel,\n",
    "                      stride=stride,\n",
    "                      pad=pad,\n",
    "                      normalisation=normalisation)         # ['bn', 'wn', else]\n",
    "conv_2D = conv_2D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "481ad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2D = conv_2D(input_2D.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "024eb74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 1, 1800])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b33f0234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 1800])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2D.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2140a297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 600])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_1D = nn.MaxPool1d(kernel_size=3, stride=3, padding=0).cuda()\n",
    "pool_result_1D = pool_1D(result_2D.squeeze())\n",
    "pool_result_1D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9700264",
   "metadata": {},
   "source": [
    "### conv2D + pool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "778e31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "conv_hidden = 16\n",
    "kernel = (68, 5)\n",
    "stride= (1, 1)\n",
    "pad = (33, 2)\n",
    "normalisation = 'bn'\n",
    "\n",
    "input_2D = torch.randn(2, 3, 68, 1800)   # (batch_size, channel, heigh(num_KPs), width(time series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b884c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2D = ConvBlock2d(in_channels=input_dim,      \n",
    "                      out_channels=conv_hidden,   \n",
    "                      kernel=kernel,\n",
    "                      stride=stride,\n",
    "                      pad=pad,\n",
    "                      normalisation=normalisation)         # ['bn', 'wn', else]\n",
    "conv_2D = conv_2D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8823c182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 67, 1800])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2D = conv_2D(input_2D.cuda())\n",
    "result_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b12c7e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 22, 600])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_2D = nn.MaxPool2d(kernel_size=3, stride=3, padding=0).cuda()\n",
    "pool_result_2D = pool_2D(result_2D)\n",
    "pool_result_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa1582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cd68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfdc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3bb0844",
   "metadata": {},
   "source": [
    "### test ConvLSTM 2D, whole class together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "57f7e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM_2D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv2D_hidden, conv1D_hidden, \n",
    "                 lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(ConvLSTM_2D, self).__init__()\n",
    "        self.conv2D = ConvBlock2d(in_channels=input_dim,\n",
    "                                  out_channels=conv2D_hidden,\n",
    "                                  kernel=(68, 5),\n",
    "                                  stride=(1, 1),\n",
    "                                  pad=(33, 2),\n",
    "                                  normalisation='bn')\n",
    "        self.conv1D = ConvBlock1d(in_channels=22*conv2D_hidden,     # (((num_kp + 2*P) - K)//S + 1) // max_pool * conv2D_hidden\n",
    "                                  out_channels=conv1D_hidden,       # 512\n",
    "                                  kernel=11,\n",
    "                                  stride=1,\n",
    "                                  pad=5,\n",
    "                                  normalisation='bn')\n",
    "        self.pool2D = nn.MaxPool2d(kernel_size=3,\n",
    "                                   stride=3,\n",
    "                                   padding=0)\n",
    "        self.pool1D = nn.MaxPool1d(kernel_size=5,\n",
    "                                   stride=5,\n",
    "                                   padding=0)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=conv1D_hidden,\n",
    "                            hidden_size=lstm_hidden,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.attention_layer = nn.Sequential(nn.Linear(lstm_hidden, lstm_hidden),\n",
    "                                             nn.ReLU(inplace=True))\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,\n",
    "                                 out_channels=output_dim,\n",
    "                                 activation=activation,\n",
    "                                 normalisation=norm)\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        m = nn.Tanh()(h)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, V, T = x.shape\n",
    "        x = self.conv2D(x)  # output x: (N, conv2D_hidden, 67, 1800)\n",
    "        x = self.pool2D(x)  # output x: (N, conv2D_hidden, 67//3, 1800//3)  2, 16, 22, 600\n",
    "        N, C, V, T = x.shape\n",
    "        x = x.view(N, C * V, T)\n",
    "        x = self.conv1D(x)  # output x: (N, conv1D_hidden, 600)\n",
    "        x = self.pool1D(x)  # output x: (N, conv1D_hidden, 600//5)\n",
    "        x = self.drop(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, (final_hidden_state, final_cell_state) = self.lstm(x)               # output x: (N, 120, lstm_hidden)\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2).contiguous()  # output final_hidden_state: (N, num_layer*num_direction, lstm_hidden)\n",
    "        x = self.attention_net_with_w(x, final_hidden_state)                   # output x: (N, lstm_hidden)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5eb4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM_2D(input_dim = 3, \n",
    "                    output_dim = 512, \n",
    "                    conv2D_hidden = 16, \n",
    "                    conv1D_hidden = 512, \n",
    "                    lstm_hidden = 512,\n",
    "                    num_layers = 4,\n",
    "                    activation = 'relu', \n",
    "                    norm = 'bn', \n",
    "                    dropout = 0.5)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78a3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 3, 68, 1800).cuda()   # (batch_size, channel, heigh(num_KPs), width(time series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b854a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(input)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606d458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245d5e6d",
   "metadata": {},
   "source": [
    "### test ConvLSTM 1D, whole class together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f9ced453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM_1D(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_hidden,\n",
    "                 lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(ConvLSTM_1D, self).__init__()\n",
    "        self.conv2D = ConvBlock2d(in_channels=input_dim,\n",
    "                                  out_channels=conv_hidden,\n",
    "                                  kernel=(68, 11),\n",
    "                                  stride=(1, 1),\n",
    "                                  pad=(0, 5),\n",
    "                                  normalisation='bn')\n",
    "        self.pool1D = nn.MaxPool1d(kernel_size=5,\n",
    "                                   stride=5,\n",
    "                                   padding=0)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(input_size=conv_hidden,\n",
    "                            hidden_size=lstm_hidden,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.attention_layer = nn.Sequential(nn.Linear(lstm_hidden, lstm_hidden),\n",
    "                                             nn.ReLU(inplace=True))\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden,\n",
    "                                 out_channels=output_dim,\n",
    "                                 activation=activation,\n",
    "                                 normalisation=norm)\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        m = nn.Tanh()(h)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, V, T = x.shape\n",
    "        x = self.conv2D(x)  # output x: (N, conv_hidden, 1, 1800)\n",
    "        x = self.pool1D(x.squeeze())  # output x: (N, conv_hidden, 1800//5)\n",
    "        x = self.drop(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, (final_hidden_state, final_cell_state) = self.lstm(x)               # output x: (N, 120, lstm_hidden)\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2).contiguous()  # output final_hidden_state: (N, num_layer*num_direction, lstm_hidden)\n",
    "        x = self.attention_net_with_w(x, final_hidden_state)                   # output x: (N, lstm_hidden)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3ece03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM_1D(input_dim = 3, \n",
    "                    output_dim = 512, \n",
    "                    conv_hidden = 16,\n",
    "                    lstm_hidden = 512,\n",
    "                    num_layers = 4,\n",
    "                    activation = 'relu', \n",
    "                    norm = 'bn', \n",
    "                    dropout = 0.5)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81ee242",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 3, 68, 1800).cuda()   # (batch_size, channel, heigh(num_KPs), width(time series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30e1dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(input)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffa92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b078be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86386c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf256093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dafe2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer.\n",
    "    Ref: He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing\n",
    "    human-level performance on imagenet classification.\" Proceedings of the\n",
    "    IEEE international conference on computer vision. 2015.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The current layer of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    if layer.weight.ndimension() == 4:\n",
    "        (n_out, n_in, height, width) = layer.weight.size()\n",
    "        n = n_in * height * width\n",
    "    elif layer.weight.ndimension() == 3:\n",
    "        (n_out, n_in, height) = layer.weight.size()\n",
    "        n = n_in * height\n",
    "    elif layer.weight.ndimension() == 2:\n",
    "        (n_out, n) = layer.weight.size()\n",
    "\n",
    "    std = math.sqrt(2. / n)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_lstm(layer):\n",
    "    \"\"\"\n",
    "    Initialises the hidden layers in the LSTM - H0 and C0.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The LSTM layer\n",
    "    \"\"\"\n",
    "    n_i1, n_i2 = layer.weight_ih_l0.size()\n",
    "    n_i = n_i1 * n_i2\n",
    "\n",
    "    std = math.sqrt(2. / n_i)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight_ih_l0.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias_ih_l0 is not None:\n",
    "        layer.bias_ih_l0.data.fill_(0.)\n",
    "\n",
    "    n_h1, n_h2 = layer.weight_hh_l0.size()\n",
    "    n_h = n_h1 * n_h2\n",
    "\n",
    "    std = math.sqrt(2. / n_h)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight_hh_l0.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias_hh_l0 is not None:\n",
    "        layer.bias_hh_l0.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_att_layer(layer):\n",
    "    \"\"\"\n",
    "    Initilise the weights and bias of the attention layer to 1 and 0\n",
    "    respectively. This is because the first iteration through the attention\n",
    "    mechanism should weight each time step equally.\n",
    "\n",
    "    Input\n",
    "        layer: torch.Tensor - The current layer of the neural network\n",
    "    \"\"\"\n",
    "    layer.weight.data.fill_(1.)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"\n",
    "    Initialize a Batchnorm layer.\n",
    "\n",
    "    Input\n",
    "        bn: torch.Tensor - The batch normalisation layer\n",
    "    \"\"\"\n",
    "\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "class ConvBlock1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a 1D convolutional layer. This includes the\n",
    "    convolutional filter but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, pad,\n",
    "                 normalisation, dil=1):\n",
    "        super(ConvBlock1d, self).__init__()\n",
    "        self.norm = normalisation\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel,\n",
    "                               stride=stride,\n",
    "                               padding=pad,\n",
    "                               dilation=dil)\n",
    "        if self.norm == 'bn':\n",
    "            self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        elif self.norm == 'wn':\n",
    "            self.conv1 = nn.utils.weight_norm(self.conv1, name='weight')\n",
    "        else:\n",
    "            self.conv1 = self.conv1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the convolutional filter\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.norm == 'bn':\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "        else:\n",
    "            x = self.relu(self.conv1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a 2D convolutional layer. This includes the\n",
    "    convolutional filter but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, pad,\n",
    "                 normalisation, att=None):\n",
    "        super(ConvBlock2d, self).__init__()\n",
    "        self.norm = normalisation\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel,\n",
    "                               stride=stride,\n",
    "                               padding=pad)\n",
    "        if self.norm == 'bn':\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        elif self.norm == 'wn':\n",
    "            self.conv1 = nn.utils.weight_norm(self.conv1, name='weight')\n",
    "        else:\n",
    "            self.conv1 = self.conv1\n",
    "        self.att = att\n",
    "        if not self.att:\n",
    "            self.act = nn.ReLU()\n",
    "        else:\n",
    "            self.norm = None\n",
    "            if self.att == 'softmax':\n",
    "                self.act = nn.Softmax(dim=-1)\n",
    "            elif self.att == 'global':\n",
    "                self.act = None\n",
    "            else:\n",
    "                self.act = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        if self.att:\n",
    "            init_att_layer(self.conv1)\n",
    "        else:\n",
    "            init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the convolutional filter\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.att:\n",
    "            x = self.conv1(x)\n",
    "            if self.act():\n",
    "                x = self.act(x)\n",
    "        else:\n",
    "            if self.norm == 'bn':\n",
    "                x = self.act(self.bn1(self.conv1(x)))\n",
    "            else:\n",
    "                x = self.act(self.conv1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an instance of a fully-connected layer. This includes the\n",
    "    hidden layers but also the type of normalisation \"batch\" or\n",
    "    \"weight\", the activation function, and initialises the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, activation, normalisation,\n",
    "                 att=None):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.att = att\n",
    "        self.norm = normalisation\n",
    "        self.fc = nn.Linear(in_features=in_channels,\n",
    "                            out_features=out_channels)\n",
    "        if activation == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "            self.norm = None\n",
    "        elif activation == 'softmax':\n",
    "            self.act = nn.Softmax(dim=-1)\n",
    "            self.norm = None\n",
    "        elif activation == 'global':\n",
    "            self.act = None\n",
    "            self.norm = None\n",
    "        else:\n",
    "            self.act = nn.ReLU()\n",
    "            if self.norm == 'bn':\n",
    "                self.bnf = nn.BatchNorm1d(out_channels)\n",
    "            elif self.norm == 'wn':\n",
    "                self.wnf = nn.utils.weight_norm(self.fc, name='weight')\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weights of the current layer\n",
    "        \"\"\"\n",
    "        if self.att:\n",
    "            init_att_layer(self.fc)\n",
    "        else:\n",
    "            init_layer(self.fc)\n",
    "        if self.norm == 'bn':\n",
    "            init_bn(self.bnf)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Passes the input through the fully-connected layer\n",
    "\n",
    "        Input\n",
    "            input: torch.Tensor - The current input at this stage of the network\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        if self.norm is not None:\n",
    "            if self.norm == 'bn':\n",
    "                x = self.act(self.bnf(self.fc(x)))\n",
    "            else:\n",
    "                x = self.act(self.wnf(x))\n",
    "        else:\n",
    "            if self.att:\n",
    "                if self.act:\n",
    "                    x = self.act(self.fc(x))\n",
    "                else:\n",
    "                    x = self.fc(x)\n",
    "            else:\n",
    "                if self.act:\n",
    "                    x = self.act(self.fc(x))\n",
    "                else:\n",
    "                    x = self.fc(x)        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLSTM_PR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, conv_hidden, lstm_hidden, num_layers, activation, norm, dropout):\n",
    "        super(ConvLSTM_PR, self).__init__()\n",
    "        self.conv = ConvBlock2d(in_channels=input_dim,      # 80\n",
    "                                out_channels=conv_hidden,   # 128\n",
    "                                kernel=(2, 3),\n",
    "                                stride=(1, 1),\n",
    "                                pad=(0, 1),\n",
    "                                normalisation='bn')         # ['bn', 'wn', else]\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3,\n",
    "                                 stride=3,\n",
    "                                 padding=0)\n",
    "        self.drop = nn.Dropout(dropout)                     # 0.2\n",
    "        self.lstm = nn.LSTM(input_size=conv_hidden,         # 128\n",
    "                            hidden_size=lstm_hidden,        # 128\n",
    "                            num_layers=num_layers,          # 2\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = FullyConnected(in_channels=lstm_hidden*2,   # 128\n",
    "                                 out_channels=output_dim,   # 2\n",
    "                                 activation=activation,     # ['sigmoid', 'softmax', 'global', else]\n",
    "                                 normalisation=norm)        # ['bn', 'wn']: nn.BatchNorm1d, nn.utils.weight_norm\n",
    "\n",
    "    def forward(self, net_input):\n",
    "        x = net_input\n",
    "        batch, C, F, T = x.shape\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x.squeeze())\n",
    "        x = self.drop(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, _ = self.lstm(x)                                 # output shape: (batch, width//stride(pool), lstm_hidden*2) 5x600x128\n",
    "        x = self.fc(x[:, -1, :].reshape(batch, -1))         # output shape: (batch, output_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0b21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 300, 2, 3)   # (batch_size, channel, heigh(num_KPs), width(time series))\n",
    "input = input.permute(0, 3, 2, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "793f5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM_PR(input_dim = 3, \n",
    "                    output_dim = 128, \n",
    "                    conv_hidden = 128,\n",
    "                    lstm_hidden = 128,\n",
    "                    num_layers = 4,\n",
    "                    activation = 'relu', \n",
    "                    norm = 'bn', \n",
    "                    dropout = 0.5)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0eaad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(input.cuda())\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a568a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b049385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5754b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efe227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
